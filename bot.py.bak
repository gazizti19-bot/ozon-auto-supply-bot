#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ozon FBO Telegram Bot + GigaChat + Auto-supplies + –ü–æ–ª–Ω—ã–π –º–∞—Å—Ç–µ—Ä –∞–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (–∫–Ω–æ–ø–æ—á–Ω—ã–π)

VERSION: stable-grounded-1.4.6-qty-fsm-fix2+autobook-full-flow-complete+envfix

–û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –ê–Ω–∞–ª–∏–∑ –æ—Å—Ç–∞—Ç–∫–æ–≤ FBO –ø–æ SKU, –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–µ—Ñ–∏—Ü–∏—Ç–æ–≤, —Ç–∞—Ä–≥–µ—Ç—ã, –ø–æ–∫—Ä—ã—Ç–∏–µ, –∏–Ω–¥–µ–∫—Å—ã (FACT).
- –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–∫–ª–∞–¥–æ–≤ –∏ –∞–≥—Ä–µ–≥–∞—Ç—ã –ø–æ —Å–∫–ª–∞–¥–∞–º/–∫–ª–∞—Å—Ç–µ—Ä–∞–º.
- –ò—Å—Ç–æ—Ä–∏—è –æ—Å—Ç–∞—Ç–∫–æ–≤ (—Å–Ω–∞–ø—à–æ—Ç—ã) + —Ä–∞—Å—á—ë—Ç –Ω–æ—Ä–º/—Ç–∞—Ä–≥–µ—Ç–æ–≤ –∏–∑ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è.
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å supply_integration / supply_watch (–µ—Å–ª–∏ –µ—Å—Ç—å) + fallback –º–∞—Å—Ç–µ—Ä –∞–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.
- AI (GigaChat) –¥–≤–∞ —Ä–µ–∂–∏–º–∞: FACT –∏ GENERAL —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π.
- –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: health/env/SSL/–∫–ª–∞–¥—Å—Ç–µ—Ä—ã/–∞–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä.
- –ñ—É—Ä–Ω–∞–ª —Å–æ–±—ã—Ç–∏–π –ø–æ—Å—Ç–∞–≤–æ–∫.
- –ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –≤–Ω–µ—à–Ω–∏–º flows.autobook_flow (–µ—Å–ª–∏ –æ–Ω —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω).

–í–∞–∂–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ (FIX):
–†–∞–Ω—å—à–µ supply_integration –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–ª—Å—è –î–û load_dotenv(), –∏–∑-–∑–∞ —á–µ–≥–æ –µ–≥–æ –ø–∞—Ç—á–∏ –≤–∏–¥–µ–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è ENV (–Ω–∞–ø—Ä. DAYS=0).
–¢–µ–ø–µ—Ä—å load_dotenv() –ø–µ—Ä–µ–Ω–µ—Å—ë–Ω –í–´–®–ï –∏–º–ø–æ—Ä—Ç–∞, –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º DAYS>=1.
"""

import os
os.environ["AUTO_BOOK"] = "0"  # –≤—Ä—É—á–Ω—É—é –æ—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–∑–∞—è–≤–∫—É –ø–∞—Ç—á–µ–º —Ç–∞–π–º—Å–ª–æ—Ç–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º –º–∞—Å—Ç–µ—Ä)
import asyncio
import logging
import json
import math
import time
import re
import base64
import uuid
import hashlib
import signal
import tempfile
import subprocess
import shlex
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
from zoneinfo import ZoneInfo
import html
import sys as _sys, os as _os

# ===== sys.path –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ =====
_ROOT_DIR = _os.path.dirname(_os.path.abspath(__file__))
if _ROOT_DIR not in _sys.path:
    _sys.path.insert(0, _ROOT_DIR)

# ===== FIX: –°–ù–ê–ß–ê–õ–ê –∑–∞–≥—Ä—É–∂–∞–µ–º .env, —á—Ç–æ–±—ã –≤–Ω–µ—à–Ω–∏–µ –º–æ–¥—É–ª–∏ –≤–∏–¥–µ–ª–∏ —Ä–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è =====
from dotenv import load_dotenv
load_dotenv()

# ===== FIX: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è DAYS –¥–æ –∏–º–ø–æ—Ä—Ç–∞ supply_integration (–ø–∞—Ç—á —á–∏—Ç–∞–µ—Ç ENV –Ω–∞–ø—Ä—è–º—É—é) =====
_raw_days = os.getenv("DAYS", "").strip()
if not _raw_days or _raw_days == "0":
    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å—Ç–∞–≤–∏–º 3, —á—Ç–æ–±—ã httpx_timeslot_patch –Ω–µ –Ω–∞—Å–ª–µ–¥–æ–≤–∞–ª 0 –¥–Ω–µ–π
    os.environ["DAYS"] = "3"

# –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –µ—Å–ª–∏ –∫—Ç–æ-—Ç–æ –±—É–¥–µ—Ç –ø–æ–∑–∂–µ –µ—ë —á–∏—Ç–∞—Ç—å —Å–Ω–æ–≤–∞
if int(os.getenv("DAYS", "3")) <= 0:
    os.environ["DAYS"] = "3"

# ===== –ò–º–ø–æ—Ä—Ç –≤–Ω–µ—à–Ω–∏—Ö –º–æ–¥—É–ª–µ–π –ø–æ—Å—Ç–∞–≤–æ–∫ (—Ç–µ–ø–µ—Ä—å —É–∂–µ –ø–æ—Å–ª–µ load_dotenv) =====
try:
    import supply_integration as si
except Exception as _e:
    si = None
    logging.getLogger("ozon-bot").warning("supply_integration not available (will use fallback only): %s", _e)

try:
    from supply_watch import register_supply_scheduler
except Exception:
    def register_supply_scheduler(*args, **kwargs):
        logging.getLogger("ozon-bot").warning("supply_watch.register_supply_scheduler not available; watcher disabled.")
        return None

try:
    from supply_watch import purge_tasks, purge_all_tasks, purge_stale_nonfinal
except Exception:
    purge_tasks = purge_all_tasks = purge_stale_nonfinal = None

# ===== External Autobook Flow =====
AUTOBOOK_ENABLED = False
_AUTOBOOK_IMPORT_ERROR: Optional[str] = None
try:
    import flows.autobook_flow as abf
    autobook_router = abf.router
    AUTOBOOK_ENABLED = True
except Exception as e:
    autobook_router = None
    AUTOBOOK_ENABLED = False
    _AUTOBOOK_IMPORT_ERROR = f"{e.__class__.__name__}: {e}"

VERSION = "stable-grounded-1.4.6-qty-fsm-fix2+autobook-full-flow-complete+envfix"

import httpx
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from aiogram import Bot, Dispatcher, F
from aiogram.types import (
    Message, CallbackQuery, ReplyKeyboardMarkup, KeyboardButton,
    InlineKeyboardMarkup, InlineKeyboardButton, FSInputFile
)
from aiogram.filters import Command
from aiogram.filters import StateFilter
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from aiogram.exceptions import TelegramRetryAfter

# ================== ENV ==================
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
OZON_CLIENT_ID = os.getenv("OZON_CLIENT_ID", "").strip()
OZON_API_KEY = os.getenv("OZON_API_KEY", "").strip()

DEFAULT_DROPOFF_ID = (os.getenv("DEFAULT_DROPOFF_ID") or os.getenv("DROP_ID") or "").strip()
DEFAULT_DROPOFF_NAME = (
    os.getenv("DEFAULT_DROPOFF_NAME")
    or os.getenv("DROP_NAME")
    or os.getenv("DEFAULT_DROP_OFF_NAME")
    or ""
).strip()

TIMEWINDOWS_RAW = os.getenv("TIMEWINDOWS", "09:00-12:00;12:00-15:00;15:00-18:00")
DAYS_ENV = int(os.getenv("DAYS", "3"))
if DAYS_ENV <= 0:
    DAYS_ENV = 3  # —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞
DISABLE_TS_FALLBACK = os.getenv("DISABLE_TS_FALLBACK", "0") == "1"

MIN_STOCK = int(os.getenv("MIN_STOCK", "100"))
TARGET_MULTIPLIER = float(os.getenv("TARGET_MULTIPLIER", "2"))
HISTORY_RETENTION_DAYS = int(os.getenv("HISTORY_RETENTION_DAYS", "120"))
HISTORY_LOOKBACK_DAYS = int(os.getenv("HISTORY_LOOKBACK_DAYS", "90"))
MIN_HISTORY_HOURS = float(os.getenv("MIN_HISTORY_HOURS", "6"))
MAX_HISTORY_POINTS = int(os.getenv("MAX_HISTORY_POINTS", "300"))
MAX_HISTORY_SNAPSHOTS = int(os.getenv("MAX_HISTORY_SNAPSHOTS", "5000"))

SNAPSHOT_INTERVAL_MINUTES = int(os.getenv("SNAPSHOT_INTERVAL_MINUTES", "30"))
SNAPSHOT_STALE_MINUTES = int(os.getenv("SNAPSHOT_STALE_MINUTES", "15"))
SNAPSHOT_MIN_REUSE_SECONDS = int(os.getenv("SNAPSHOT_MIN_REUSE_SECONDS", "120"))
HISTORY_PRUNE_EVERY_MINUTES = int(os.getenv("HISTORY_PRUNE_EVERY_MINUTES", "360"))

DAILY_NOTIFY_HOUR = int(os.getenv("DAILY_NOTIFY_HOUR", "9"))
DAILY_NOTIFY_MINUTE = int(os.getenv("DAILY_NOTIFY_MINUTE", "0"))
TZ_NAME = os.getenv("TZ", "UTC")

API_TIMEOUT_SECONDS = int(os.getenv("API_TIMEOUT_SECONDS", "15"))
HEALTH_WARN_LATENCY_MS = int(os.getenv("HEALTH_WARN_LATENCY_MS", "4000"))
SAVE_BUFFER_FLUSH_SECONDS = int(os.getenv("SAVE_BUFFER_FLUSH_SECONDS", "30"))

DEFAULT_VIEW_MODE = "FULL"

LLM_PROVIDER = os.getenv("LLM_PROVIDER", "").lower().strip()
GIGACHAT_CLIENT_ID_RAW = os.getenv("GIGACHAT_CLIENT_ID", "")
GIGACHAT_CLIENT_SECRET_RAW = os.getenv("GIGACHAT_CLIENT_SECRET", "")
GIGACHAT_CLIENT_ID = GIGACHAT_CLIENT_ID_RAW.strip()
GIGACHAT_CLIENT_SECRET = GIGACHAT_CLIENT_SECRET_RAW.strip()
GIGACHAT_SCOPE = os.getenv("GIGACHAT_SCOPE", "GIGACHAT_API_B2B").strip()
GIGACHAT_TOKEN_URL = os.getenv("GIGACHAT_TOKEN_URL", "https://ngw.devices.sberbank.ru:9443/api/v2/oauth").strip()
GIGACHAT_API_URL = os.getenv("GIGACHAT_API_URL", "https://gigachat.devices.sberbank.ru/api/v1/chat/completions").strip()
GIGACHAT_MODEL = os.getenv("GIGACHAT_MODEL", "GigaChat").strip()
GIGACHAT_TEMPERATURE = float(os.getenv("GIGACHAT_TEMPERATURE", "0.3"))
GIGACHAT_MAX_TOKENS = int(os.getenv("GIGACHAT_MAX_TOKENS", "800"))
GIGACHAT_TIMEOUT_SECONDS = int(os.getenv("GIGACHAT_TIMEOUT_SECONDS", "40"))
GIGACHAT_VERIFY_SSL = os.getenv("GIGACHAT_VERIFY_SSL", "1") != "0"
GIGACHAT_SSL_MODE = os.getenv("GIGACHAT_SSL_MODE", "auto").lower().strip()
GIGACHAT_CA_CERT = os.getenv("GIGACHAT_CA_CERT", "/app/ca/gigachat_ca.pem").strip()
GIGACHAT_TOKEN_CACHE_ENV = os.getenv("GIGACHAT_TOKEN_CACHE", "keys/gigachat_token_cache.json")

LLM_FORCE_FACT_MODE = os.getenv("LLM_FORCE_FACT_MODE", "1") == "1"
AI_MIN_INTERVAL_SECONDS = int(os.getenv("AI_MIN_INTERVAL_SECONDS", "5"))
LLM_TOP_DEFICITS = int(os.getenv("LLM_TOP_DEFICITS", "20"))
LLM_TOP_WAREHOUSES = int(os.getenv("LLM_TOP_WAREHOUSES", "8"))
LLM_TOP_CLUSTERS = int(os.getenv("LLM_TOP_CLUSTERS", "8"))
LLM_MAX_CONTEXT_SKU = int(os.getenv("LLM_MAX_CONTEXT_SKU", "10"))
LLM_MAX_CONTEXT_WAREHOUSE = int(os.getenv("LLM_MAX_CONTEXT_WAREHOUSE", "6"))
LLM_INVENTORY_SAMPLE_SKU = int(os.getenv("LLM_INVENTORY_SAMPLE_SKU", "50"))
LLM_FULL_DETAIL_SKU = int(os.getenv("LLM_FULL_DETAIL_SKU", "25"))
LLM_FULL_DETAIL_WAREHOUSES = int(os.getenv("LLM_FULL_DETAIL_WAREHOUSES", "6"))
LLM_FACT_SOFT_LIMIT_CHARS = int(os.getenv("LLM_FACT_SOFT_LIMIT_CHARS", "18000"))
LLM_ENABLE_ANSWER_CACHE = os.getenv("LLM_ENABLE_ANSWER_CACHE", "1") == "1"
LLM_STYLE_ENABLED = os.getenv("LLM_STYLE_ENABLED", "1") == "1"

LLM_GENERAL_TEMPERATURE = float(os.getenv("LLM_GENERAL_TEMPERATURE", "0.7"))
GENERAL_HISTORY_MAX = int(os.getenv("GENERAL_HISTORY_MAX", "12"))
DEFAULT_CHAT_MODE = os.getenv("DEFAULT_CHAT_MODE", "fact").lower().strip()

DIAG_TOP_DEFICITS = int(os.getenv("DIAG_TOP_DEFICITS", "8"))
DIAG_TOP_WAREHOUSES = int(os.getenv("DIAG_TOP_WAREHOUSES", "6"))
DIAG_TOP_CLUSTERS = int(os.getenv("DIAG_TOP_CLUSTERS", "6"))

WAREHOUSE_CLUSTERS_ENV = os.getenv("WAREHOUSE_CLUSTERS", "").strip()

STOCK_PAGE_SIZE = min(25, max(5, int(os.getenv("STOCK_PAGE_SIZE", "40"))))
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
SUPPLY_JOB_INTERVAL = int(os.getenv("SUPPLY_JOB_INTERVAL", "45"))

logging.basicConfig(
    level=getattr(logging, LOG_LEVEL, logging.INFO),
    format="%(asctime)s %(levelname)s:%(name)s: %(message)s"
)
log = logging.getLogger("ozon-bot")
if not AUTOBOOK_ENABLED and _AUTOBOOK_IMPORT_ERROR:
    log.warning("Autobook external disabled: %s", _AUTOBOOK_IMPORT_ERROR)

DATA_DIR = Path(os.getenv("DATA_DIR", "data"))
DATA_DIR.mkdir(parents=True, exist_ok=True)
STATE_FILE = DATA_DIR / "bot_state.json"
CACHE_FILE = DATA_DIR / "sku_cache.json"
HISTORY_FILE = DATA_DIR / "stock_history.json"
KEYS_DIR = DATA_DIR / "keys"; KEYS_DIR.mkdir(exist_ok=True)
GIGACHAT_TOKEN_CACHE_FILE = (DATA_DIR / GIGACHAT_TOKEN_CACHE_ENV).resolve()
SUPPLY_EVENTS_FILE = DATA_DIR / "supply_events.json"

if not TELEGRAM_BOT_TOKEN:
    raise SystemExit("Missing TELEGRAM_BOT_TOKEN")

MOCK_MODE = not (OZON_CLIENT_ID and OZON_API_KEY)
GIGACHAT_ENABLED = (LLM_PROVIDER == "gigachat")

bot = Bot(token=TELEGRAM_BOT_TOKEN)
dp = Dispatcher(storage=MemoryStorage())

ADMIN_ID: Optional[int] = None
SKU_NAME_CACHE: Dict[int, str] = {}
BOT_STATE: Dict[str, Any] = {}
LAST_DEFICIT_CACHE: Dict[int, Dict[str, Any]] = {}
HISTORY_CACHE: List[dict] = []
LAST_SNAPSHOT_TS = 0
ANALYZE_LOCK = asyncio.Lock()
LAST_API_LATENCY_MS = 0.0
LAST_ANALYZE_MS = 0.0
LAST_ANALYZE_ERROR: Optional[str] = None
_HISTORY_DIRTY = False
_LAST_SAVE_FLUSH = 0.0
_GIGACHAT_TOKEN_MEM: Dict[str, Any] = {}
_LAST_AI_CALL = 0.0
FACT_INDEX: Dict[str, Any] = {}
ANSWER_CACHE: Dict[str, str] = {}
GENERAL_HISTORY: Dict[int, List[Dict[str, str]]] = {}
SUPPLY_EVENTS: Dict[str, List[Dict[str, Any]]] = {}

ENV_SKU = os.getenv("SKU_LIST", "")
if ENV_SKU:
    try:
        SKU_LIST = [int(s.strip()) for s in ENV_SKU.replace(";", ",").split(",") if s.strip()]
    except Exception:
        SKU_LIST = []
else:
    SKU_LIST = []

# ===== Cluster Patterns =====
CLUSTER_MAP: Dict[str, str] = {}
RAW_CLUSTER_PATTERNS: Dict[str, List[str]] = {
    "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥ –∏ –°–ó–û": [r"—Å–∞–Ω–∫—Ç", r"–ø–∏—Ç–µ—Ä", r"\b—Å–ø–±\b", r"\b—Å–∑–æ\b", r"–ª–µ–Ω–∏–Ω–≥—Ä"],
    "–ö–∞–∑–∞–Ω—å": [r"–∫–∞–∑–∞–Ω"],
    "–°–∞–º–∞—Ä–∞": [r"—Å–∞–º–∞—Ä"],
    "–£—Ñ–∞": [r"\–±—É—Ñ–∞\b"],
    "–Æ–≥": [r"\b—é–≥\b", r"—é–∂–Ω", r"—Ä–æ—Å—Ç–æ–≤", r"–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä", r"–∞—Å—Ç—Ä–∞—Ö–∞–Ω"],
    "–í–æ—Ä–æ–Ω–µ–∂": [r"–≤–æ—Ä–æ–Ω–µ–∂"],
    "–°–∞—Ä–∞—Ç–æ–≤": [r"—Å–∞—Ä–∞—Ç–æ–≤"],
    "–ö–∞–≤–∫–∞–∑": [r"–∫–∞–≤–∫–∞–∑", r"—á–µ—Ä–∫–µ—Å", r"—Å—Ç–∞–≤—Ä–æ–ø", r"–¥–∞–≥–µ—Å—Ç", r"–æ—Å–µ—Ç", r"–∏–Ω–≥—É—à", r"—á–µ—á", r"–º–∞—Ö–∞—á–∫–∞–ª"],
    "–ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫": [r"–∫—Ä–∞—Å–Ω–æ—è—Ä"],
    "–°–∏–±–∏—Ä—å": [r"—Å–∏–±–∏—Ä", r"—Ç–æ–º—Å–∫", r"–æ–º—Å–∫", r"–Ω–æ–≤–æ—Å–∏–±", r"–∫–µ–º–µ—Ä–æ–≤", r"–±–∞—Ä–Ω–∞—É–ª", r"–∏—Ä–∫—É—Ç", r"–∫—É–∑–±–∞—Å"],
    "–£—Ä–∞–ª": [r"—É—Ä–∞–ª", r"–µ–∫–∞—Ç–µ—Ä–∏–Ω", r"—á–µ–ª—è–±", r"–ø–µ—Ä–º—å", r"—Å–≤–µ—Ä–¥–ª–æ–≤"],
    "–¢—é–º–µ–Ω—å": [r"—Ç—é–º–µ–Ω—å"],
    "–î–∞–ª—å–Ω–∏–π –í–æ—Å—Ç–æ–∫": [r"–¥–∞–ª—å–Ω(–∏–π)?\s*–≤–æ—Å—Ç", r"–≤–ª–∞–¥–∏–≤–æ—Å—Ç", r"—Ö–∞–±–∞—Ä–æ–≤", r"–∫–∞–º—á–∞—Ç", r"—Å–∞—Ö–∞–ª", r"–º–∞–≥–∞–¥–∞–Ω", r"—è–∫—É—Ç", r"–ø—Ä–∏–º–æ—Ä"],
    "–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥": [r"–∫–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥"],
    "–Ø—Ä–æ—Å–ª–∞–≤–ª—å": [r"—è—Ä–æ—Å–ª–∞–≤"],
    "–ë–µ–ª–∞—Ä—É—Å—å": [r"–±–µ–ª–∞—Ä—É—Å", r"–º–∏–Ω—Å–∫", r"\b—Ä–±\b"],
    "–ö–∞–∑–∞—Ö—Å—Ç–∞–Ω": [r"–∫–∞–∑–∞—Ö—Å—Ç–∞–Ω", r"–∞–ª–º–∞—Ç—ã", r"–∞—Å—Ç–∞–Ω", r"\b–∫–∑\b", r"–∫–∞—Ä–∞–≥–∞–Ω–¥–∞"],
    "–ê—Ä–º–µ–Ω–∏—è": [r"–∞—Ä–º–µ–Ω–∏", r"–µ—Ä–µ–≤–∞–Ω"],
}
CLUSTER_PATTERN_MAP: Dict[str, List[re.Pattern]] = {
    cname: [re.compile(p, re.IGNORECASE) for p in pats]
    for cname, pats in RAW_CLUSTER_PATTERNS.items()
}

def parse_cluster_env():
    global CLUSTER_MAP
    raw = WAREHOUSE_CLUSTERS_ENV
    if not raw:
        CLUSTER_MAP = {}
        return
    try:
        obj=json.loads(raw)
        if isinstance(obj,dict):
            CLUSTER_MAP={str(k):str(v) for k,v in obj.items()}
            return
    except Exception:
        pass
    mapping={}
    for part in raw.split(";"):
        part=part.strip()
        if not part: continue
        if "=" in part:
            k,v=part.split("=",1)
            mapping[str(k.strip())]=v.strip().strip('"').strip("'")
    CLUSTER_MAP=mapping
parse_cluster_env()

# ===== UI CONST =====
GR_FILL = {"red":"üü•","orange":"üüß","yellow":"üü®","green":"üü©"}
EMPTY_SEG="‚ñ´"
BAR_LEN=12
SEP_THIN="‚îÄ"*60
SEP_BOLD="‚ïê"*60

EMOJI_OK="‚úÖ"; EMOJI_WARN="‚ö†"; EMOJI_ANALYZE="üîç"; EMOJI_NOTIFY="üì£"; EMOJI_BOX="üì¶"
EMOJI_WH="üè¨"; EMOJI_CLUSTER="üó∫"; EMOJI_REFRESH="üîÑ"; EMOJI_SETTINGS="‚öô"; EMOJI_TARGET="üéØ"
EMOJI_INFO="‚Ñπ"; EMOJI_DIAG="üß™"; EMOJI_AI="ü§ñ"; EMOJI_PERF="‚è±"; EMOJI_INV="üìä"; EMOJI_CLOUD="‚òÅ"
EMOJI_CHAT="üí¨"; EMOJI_LIST="üìÑ"

LEGEND_TEXT="–õ–µ–≥–µ–Ω–¥–∞: üü• <25%  üüß <50%  üü® <80%  üü© ‚â•80%"
AI_MAX_RENDER_LINES=420

# ===== FSM =====
class AIChatState(StatesGroup):
    waiting=State()

class AutobookStates(StatesGroup):
    picking_sku=State()
    enter_qty=State()
    choose_warehouse=State()
    choose_date=State()
    choose_timeslot=State()
    confirm=State()
    creating=State()

# ===== Persistence & State Utils =====
def ensure_admin(uid:int):
    global ADMIN_ID
    if ADMIN_ID is None:
        ADMIN_ID=uid

def build_html(lines:List[str])->str:
    return html.escape("\n".join(lines)).replace("¬ß¬ßB¬ß¬ß","<b>").replace("¬ß¬ßEB¬ß¬ß","</b>")

def _atomic_write(path:Path, text:str):
    fd,tmp=tempfile.mkstemp(dir=str(path.parent), prefix=path.name, suffix=".tmp")
    try:
        with os.fdopen(fd,"w",encoding="utf-8") as f:
            f.write(text); f.flush(); os.fsync(f.fileno())
        os.replace(tmp,path)
    except Exception:
        try: os.unlink(tmp)
        except Exception: pass

def load_state():
    global BOT_STATE, SUPPLY_EVENTS
    if STATE_FILE.exists():
        try:
            BOT_STATE=json.loads(STATE_FILE.read_text("utf-8"))
        except Exception:
            BOT_STATE={}
    BOT_STATE.setdefault("view_mode", DEFAULT_VIEW_MODE)
    BOT_STATE.setdefault("style_enabled", LLM_STYLE_ENABLED)
    BOT_STATE.setdefault("chat_mode", DEFAULT_CHAT_MODE if DEFAULT_CHAT_MODE in ("fact","general") else "fact")
    if SUPPLY_EVENTS_FILE.exists():
        try:
            SUPPLY_EVENTS.update(json.loads(SUPPLY_EVENTS_FILE.read_text("utf-8")))
        except Exception:
            pass

def save_state():
    try:
        _atomic_write(STATE_FILE, json.dumps(BOT_STATE, ensure_ascii=False, indent=2))
    except Exception as e:
        log.warning("save_state error: %s", e)

def load_cache():
    global SKU_NAME_CACHE
    if CACHE_FILE.exists():
        try:
            data=json.loads(CACHE_FILE.read_text("utf-8"))
            SKU_NAME_CACHE={int(k):v for k,v in data.items()}
        except Exception:
            SKU_NAME_CACHE={}

def save_cache_if_needed(prev:int):
    if len(SKU_NAME_CACHE)>prev:
        try:
            _atomic_write(CACHE_FILE, json.dumps(SKU_NAME_CACHE, ensure_ascii=False, indent=2))
        except Exception as e:
            log.warning("cache save error: %s", e)

def load_history():
    global HISTORY_CACHE, LAST_SNAPSHOT_TS
    if HISTORY_FILE.exists():
        try:
            arr=json.loads(HISTORY_FILE.read_text("utf-8"))
            if isinstance(arr,list): HISTORY_CACHE[:]=arr
        except Exception as e:
            log.warning("history load error: %s", e)
    if HISTORY_CACHE:
        LAST_SNAPSHOT_TS=max(s.get("ts",0) for s in HISTORY_CACHE)

def mark_history_dirty():
    global _HISTORY_DIRTY
    _HISTORY_DIRTY=True

async def flush_history_if_needed(force=False):
    global _HISTORY_DIRTY, _LAST_SAVE_FLUSH
    if not _HISTORY_DIRTY and not force: return
    now=time.time()
    if force or (now-_LAST_SAVE_FLUSH>SAVE_BUFFER_FLUSH_SECONDS):
        try:
            await asyncio.to_thread(_atomic_write, HISTORY_FILE, json.dumps(HISTORY_CACHE, ensure_ascii=False))
            _HISTORY_DIRTY=False
            _LAST_SAVE_FLUSH=now
        except Exception as e:
            log.warning("history flush error: %s", e)

def prune_history():
    cutoff=int(time.time())-HISTORY_RETENTION_DAYS*86400
    before=len(HISTORY_CACHE)
    if not before: return
    pruned=[s for s in HISTORY_CACHE if s.get("ts",0)>=cutoff]
    if len(pruned)>MAX_HISTORY_SNAPSHOTS:
        pruned=pruned[-MAX_HISTORY_SNAPSHOTS:]
    if len(pruned)!=before:
        HISTORY_CACHE[:]=pruned
        mark_history_dirty()
        log.info("History pruned %d -> %d", before, len(pruned))

def append_snapshot(rows:List[Dict]):
    global LAST_SNAPSHOT_TS
    ts=int(time.time()); nr=[]
    for r in rows:
        try:
            sku=int(r.get("sku") or 0)
            if not sku: continue
            wid_raw=r.get("warehouse_id")
            wname=(r.get("warehouse_name") or (r.get("warehouse") or {}).get("name") or (str(wid_raw) if wid_raw else "–°–∫–ª–∞–¥"))
            qty=int(r.get("free_to_sell_amount") or 0)
            if qty<0: qty=0
            wkey=str(wid_raw) if wid_raw not in (None,"") else f"name:{wname}"
            nr.append({"sku":sku,"warehouse_key":wkey,"warehouse_name":wname,"qty":qty})
        except Exception:
            continue
    HISTORY_CACHE.append({"ts":ts,"rows":nr})
    LAST_SNAPSHOT_TS=ts
    mark_history_dirty()

def _supply_log_append(chat_id:int, entry:Dict[str,Any]):
    arr=SUPPLY_EVENTS.setdefault(str(chat_id), [])
    arr.append(entry)
    if len(arr)>300:
        del arr[0:len(arr)-300]
    try:
        _atomic_write(SUPPLY_EVENTS_FILE, json.dumps(SUPPLY_EVENTS, ensure_ascii=False, indent=2))
    except Exception as e:
        log.warning("supply events save error: %s", e)

# ===== Ozon API =====
async def ozon_stock_fbo(skus:List[int])->Tuple[List[Dict],Optional[str]]:
    if not skus: return [], "SKU_LIST –ø—É—Å—Ç"
    if MOCK_MODE:
        demo_wh=[(1,"–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥ –§–ë–û"),(2,"–ö–∞–∑–∞–Ω—å"),(3,"–°–∞–º–∞—Ä–∞"),(4,"–£—Ñ–∞"),(5,"–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É"),
                 (6,"–í–æ—Ä–æ–Ω–µ–∂"),(7,"–°–∞—Ä–∞—Ç–æ–≤"),(8,"–ú–∞—Ö–∞—á–∫–∞–ª–∞"),(9,"–ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫"),(10,"–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫")]
        rows=[]
        for sku in skus:
            base=(sku%50)+20
            for wid,name in demo_wh:
                rows.append({"sku":sku,"warehouse_id":wid,"warehouse_name":name,
                             "free_to_sell_amount":max(0, base - (wid*2) + (sku%7))})
        return rows, None
    url="https://api-seller.ozon.ru/v2/analytics/stock_on_warehouses"
    payload={"sku":skus,"limit":1000,"offset":0}
    headers={"Client-Id":OZON_CLIENT_ID,"Api-Key":OZON_API_KEY,"Content-Type":"application/json"}
    start=time.time()
    try:
        async with httpx.AsyncClient(timeout=API_TIMEOUT_SECONDS) as client:
            resp=await client.post(url,json=payload,headers=headers)
    except Exception as e:
        return [], f"HTTP error: {e}"
    finally:
        global LAST_API_LATENCY_MS
        LAST_API_LATENCY_MS=(time.time()-start)*1000
    if resp.status_code!=200:
        try: data=resp.json(); msg=data.get("message") or data.get("error") or resp.text
        except Exception: msg=resp.text
        return [], f"Ozon API {resp.status_code}: {msg}"
    try: data=resp.json()
    except Exception: return [], "Non-JSON response"
    rows=[]
    if isinstance(data,dict):
        res=data.get("result")
        if isinstance(res,dict) and isinstance(res.get("rows"),list):
            rows=res["rows"]
        elif isinstance(data.get("rows"),list):
            rows=data["rows"]
        else:
            for v in data.values():
                if isinstance(v,list) and v and isinstance(v[0],dict):
                    rows=v; break
    return rows or [], None

async def ozon_product_names_by_sku(skus:List[int])->Tuple[Dict[int,str],Optional[str]]:
    if not skus: return {}, None
    if MOCK_MODE:
        return {s:f"Demo SKU {s}" for s in skus}, None
    headers={"Client-Id":OZON_CLIENT_ID,"Api-Key":OZON_API_KEY,"Content-Type":"application/json"}
    mapping={}
    # v3
    try:
        async with httpx.AsyncClient(timeout=API_TIMEOUT_SECONDS) as client:
            resp=await client.post("https://api-seller.ozon.ru/v3/product/info/list", json={"sku":skus,"product_id":[]}, headers=headers)
            if resp.status_code==200:
                data=resp.json(); result=data.get("result")
                cand=[]
                if isinstance(result,list): cand=result
                elif isinstance(result,dict):
                    for k in ("items","products"):
                        if isinstance(result.get(k),list): cand=result[k]; break
                for item in cand or []:
                    if not isinstance(item,dict): continue
                    sku_v=item.get("sku") or item.get("offer_id") or item.get("id")
                    name_v=item.get("name") or item.get("title") or item.get("display_name") or item.get("product_name")
                    try:
                        sku_i=int(sku_v)
                        if name_v: mapping[sku_i]=name_v
                    except Exception:
                        continue
    except Exception:
        pass
    missing=[s for s in skus if s not in mapping]
    # v2 sku
    if missing:
        try:
            async with httpx.AsyncClient(timeout=API_TIMEOUT_SECONDS) as client:
                resp=await client.post("https://api-seller.ozon.ru/v2/product/info/list",
                                       json={"sku":missing,"product_id":[],"offer_id":[]}, headers=headers)
                if resp.status_code==200:
                    data=resp.json(); res=data.get("result") or {}
                    items=res.get("items") or res.get("products") or []
                    for it in items or []:
                        sku_v=it.get("sku") or it.get("offer_id") or it.get("id")
                        name_v=it.get("name") or it.get("title") or it.get("display_name") or it.get("product_name")
                        try:
                            sku_i=int(sku_v)
                            if name_v: mapping[sku_i]=name_v
                        except Exception:
                            continue
        except Exception:
            pass
    missing=[s for s in skus if s not in mapping]
    # v2 offer_id fallback
    if missing:
        try:
            async with httpx.AsyncClient(timeout=API_TIMEOUT_SECONDS) as client:
                resp=await client.post("https://api-seller.ozon.ru/v2/product/info/list",
                                       json={"offer_id":[str(s) for s in missing],"product_id":[],"sku":[]}, headers=headers)
                if resp.status_code==200:
                    data=resp.json(); res=data.get("result") or {}
                    items=res.get("items") or res.get("products") or []
                    for it in items or []:
                        sku_v=it.get("sku") or it.get("offer_id") or it.get("id")
                        name_v=it.get("name") or it.get("title") or it.get("display_name") or it.get("product_name")
                        try:
                            sku_i=int(sku_v)
                            if name_v: mapping[sku_i]=name_v
                        except Exception:
                            continue
        except Exception:
            pass
    for s in skus:
        mapping.setdefault(s, f"SKU {s}")
    return mapping, None

def skus_needing_names()->List[int]:
    return [s for s in SKU_LIST
            if (s not in SKU_NAME_CACHE) or SKU_NAME_CACHE[s]==f"SKU {s}" or SKU_NAME_CACHE[s].lower().startswith("demo sku")]

# ===== Norms / Aggregation / Coverage =====
def build_consumption_cache()->Dict[Tuple[int,str],Dict[str,Any]]:
    now=int(time.time()); cutoff=now-HISTORY_LOOKBACK_DAYS*86400
    series={}
    for snap in HISTORY_CACHE:
        ts=snap.get("ts",0)
        if ts<cutoff: continue
        for r in snap.get("rows",[]):
            sku=r.get("sku"); wkey=r.get("warehouse_key"); qty=r.get("qty")
            if sku is None or wkey is None: continue
            try: sku_i=int(sku); qty_i=int(qty)
            except Exception: continue
            series.setdefault((sku_i,wkey),[]).append((ts,qty_i))
    cache={}
    for key,arr in series.items():
        arr.sort(key=lambda x:x[0])
        if MAX_HISTORY_POINTS>0 and len(arr)>MAX_HISTORY_POINTS:
            arr=arr[-MAX_HISTORY_POINTS:]
        points=len(arr); total_decrease=0
        if points>=2:
            span=arr[-1][0]-arr[0][0]
            if span>0:
                span_hours=span/3600
                for i in range(1,points):
                    p=arr[i-1][1]; c=arr[i][1]
                    if p>c: total_decrease+=p-c
                if span_hours>=MIN_HISTORY_HOURS and total_decrease>0:
                    avg_per_hour=total_decrease/span_hours
                    monthly=avg_per_hour*24*30
                    norm=max(1, math.ceil(monthly))
                    target=max(norm+1, math.ceil(norm*TARGET_MULTIPLIER))
                    cache[key]={"norm":norm,"target":target,"history_used":True}
                    continue
        norm=MIN_STOCK; target=int(MIN_STOCK*TARGET_MULTIPLIER)
        cache[key]={"norm":norm,"target":target,"is_low":False,"history_used":False}
    return cache

def evaluate_position_cached(sku:int,wkey:str,qty:int,ccache:Dict[Tuple[int,str],Dict[str,Any]])->Dict[str,Any]:
    meta=ccache.get((sku,wkey))
    if not meta:
        norm=MIN_STOCK; target=int(MIN_STOCK*TARGET_MULTIPLIER)
        return {"norm":norm,"target":target,"is_low":qty<norm,"need":max(0,target-qty) if qty<norm else 0,"history_used":False}
    norm=meta["norm"]; target=meta["target"]; is_low=qty<norm
    return {"norm":norm,"target":target,"is_low":is_low,"need":max(0,target-qty) if is_low else 0,"history_used":meta["history_used"]}

def aggregate_rows(rows:List[Dict])->Dict[int,Dict[str,Dict[str,Any]]]:
    agg={}
    for r in rows:
        try:
            sku=int(r.get("sku") or 0)
            if sku==0: continue
            qty=int(r.get("free_to_sell_amount") or r.get("qty") or 0)
            if qty<0: qty=0
            wid_raw=r.get("warehouse_id")
            wname=r.get("warehouse_name") or (r.get("warehouse") or {}).get("name") or (str(wid_raw) if wid_raw else "–°–∫–ª–∞–¥")
            wkey=str(wid_raw) if wid_raw not in (None,"") else f"name:{wname}"
        except Exception:
            continue
        agg.setdefault(sku,{})
        agg[sku].setdefault(wkey,{"qty":0,"warehouse_name":wname})
        agg[sku][wkey]["qty"]+=qty
    return agg

def coverage_bar(r:float)->Tuple[str,str]:
    if r<0: r=0
    if r<0.25: c=GR_FILL["red"]; sev="–ö–†–ò–¢"
    elif r<0.5: c=GR_FILL["orange"]; sev="–ö–†–ò–¢"
    elif r<0.8: c=GR_FILL["yellow"]; sev="–ù–ò–ó–ö–û"
    else: c=GR_FILL["green"]; sev="OK"
    filled=min(BAR_LEN,max(0,round(r*BAR_LEN)))
    bar=c*filled+EMPTY_SEG*(BAR_LEN-filled)
    return f"{bar} {int(r*100):02d}%", sev

def generate_deficit_report(rows:List[Dict], name_map:Dict[int,str], ccache:Dict[Tuple[int,str],Dict[str,Any]])->Tuple[str,List[dict]]:
    agg=aggregate_rows(rows)
    deficits={}; flat=[]
    for sku,wmap in agg.items():
        for wkey,info in wmap.items():
            qty=info["qty"]
            st=evaluate_position_cached(sku,wkey,qty,ccache)
            if st["is_low"]:
                cov=qty/st["norm"] if st["norm"] else 0
                d={"sku":sku,"name":name_map.get(sku,f"SKU {sku}"),"warehouse_key":wkey,"warehouse_name":info["warehouse_name"],
                   "qty":qty,"norm":st["norm"],"target":st["target"],"need":st["need"],
                   "coverage":cov,"history_used":st["history_used"]}
                deficits.setdefault(sku,[]).append(d)
                flat.append(d)
    if not deficits:
        return f"{EMOJI_OK} –ù–µ—Ç —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∏–∂–µ –Ω–æ—Ä–º—ã.", []
    sku_order=sorted(deficits.keys(), key=lambda s: min(x["coverage"] for x in deficits[s]))
    view_mode=BOT_STATE.get("view_mode", DEFAULT_VIEW_MODE)
    full=(view_mode=="FULL")
    crit=mid=hi=0
    lines=[f"{EMOJI_ANALYZE} ¬ß¬ßB¬ß¬ß–î–ï–§–ò–¶–ò–¢–ù–´–ï –ü–û–ó–ò–¶–ò–ò¬ß¬ßEB¬ß¬ß", LEGEND_TEXT, SEP_BOLD]
    for sku in sku_order:
        items=deficits[sku]; items.sort(key=lambda x:x["coverage"])
        pname=items[0]["name"]; worst=min(i["coverage"] for i in items)
        head=EMOJI_WARN if worst<0.5 else "‚û§"
        lines.append(f"{head} <b>{html.escape(pname)} (SKU {sku})</b>")
        total_qty=sum(i["qty"] for i in items); total_need=sum(i["need"] for i in items)
        for i in items:
            bar,sev=coverage_bar(i["coverage"])
            if i["coverage"]<0.5: crit+=1
            elif i["coverage"]<0.8: mid+=1
            else: hi+=1
            hist="hist" if i["history_used"] else "min"
            if full:
                lines.append(f"‚Ä¢ {html.escape(i['warehouse_name'])}: {i['qty']} | norm {i['norm']} | target {i['target']} ‚Üí +{i['need']}\n  {bar} {sev} [{hist}]")
            else:
                lines.append(f"‚Ä¢ {html.escape(i['warehouse_name'])}: {i['qty']} ‚Üí +{i['need']} {bar}")
        lines.append(f"  Œ£ qty={total_qty} need={total_need}")
        lines.append(SEP_THIN)
    lines.append(f"{EMOJI_TARGET} –ò—Ç–æ–≥: SKU={len(deficits)} —Å—Ç—Ä–æ–∫={len(flat)} <50%={crit} 50‚Äì80%={mid} ‚â•80%<norm={hi} —Ä–µ–∂–∏–º={view_mode}")
    return build_html(lines), flat

def filter_deficit(flat:List[dict], mode:str)->List[dict]:
    if mode=="crit": return [d for d in flat if d["coverage"]<0.5]
    if mode=="mid": return [d for d in flat if 0.5<=d["coverage"]<0.8]
    return flat

def rebuild_filtered_report(flat:List[dict], mode:str, view_mode:str)->str:
    if not flat: return build_html([f"{EMOJI_OK} –ù–µ—Ç –¥–µ—Ñ–∏—Ü–∏—Ç–æ–≤."])
    f2=filter_deficit(flat, mode)
    if not f2:
        label={"crit":"–∫—Ä–∏—Ç–∏—á–Ω—ã—Ö(<50%)","mid":"50‚Äì80%"}[mode]
        return build_html([f"{EMOJI_OK} –ù–µ—Ç –ø–æ–∑–∏—Ü–∏–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {label}."])
    per={}
    for d in f2: per.setdefault(d["sku"],[]).append(d)
    sku_order=sorted(per.keys(), key=lambda s: min(x["coverage"] for x in per[s]))
    full=(view_mode=="FULL")
    lines=[f"{EMOJI_ANALYZE} ¬ß¬ßB¬ß¬ß–§–∏–ª—å—Ç—Ä: {mode}¬ß¬ßEB¬ß¬ß", SEP_BOLD]
    for sku in sku_order:
        items=per[sku]; items.sort(key=lambda x:x["coverage"])
        pname=items[0]["name"]
        lines.append(f"‚û§ <b>{html.escape(pname)} (SKU {sku})</b>")
        total_need=sum(i["need"] for i in items); total_qty=sum(i["qty"] for i in items)
        for i in items:
            bar,sev=coverage_bar(i["coverage"])
            if full:
                lines.append(f"‚Ä¢ {html.escape(i['warehouse_name'])}: {i['qty']} | norm {i['norm']} | target {i['target']} ‚Üí +{i['need']}\n  {bar} {sev}")
            else:
                lines.append(f"‚Ä¢ {html.escape(i['warehouse_name'])}: {i['qty']} ‚Üí +{i['need']} {bar}")
        lines.append(f"  Œ£ qty={total_qty} need={total_need}")
        lines.append(SEP_THIN)
    lines.append(f"{EMOJI_TARGET} –ü–æ–∫–∞–∑–∞–Ω–æ SKU={len(per)} —Å—Ç—Ä–æ–∫={len(f2)} —Ä–µ–∂–∏–º={view_mode}")
    return build_html(lines)

def deficit_filters_kb()->InlineKeyboardMarkup:
    rows=[
        [InlineKeyboardButton(text="–í—Å–µ", callback_data="filter:all"),
         InlineKeyboardButton(text="–ö—Ä–∏—Ç–∏—á–Ω–æ", callback_data="filter:crit"),
         InlineKeyboardButton(text="50‚Äì80%", callback_data="filter:mid")],
        [InlineKeyboardButton(text=f"{EMOJI_REFRESH} –û–±–Ω–æ–≤–∏—Ç—å", callback_data="action:reanalyze")],
        [InlineKeyboardButton(text="–ê–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ", callback_data="menu_autobook")],
    ]
    return InlineKeyboardMarkup(inline_keyboard=rows)

# ===== Fallback –ê–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (–º–∞—Å—Ç–µ—Ä) =====
if not AUTOBOOK_ENABLED:
    def _parse_timewindows(raw:str)->List[str]:
        arr=[x.strip() for x in raw.split(";") if x.strip()]
        valid=[]
        for w in arr:
            if re.match(r"^\d{2}:\d{2}\s*-\s*\d{2}:\d{2}$", w):
                valid.append(w.replace(" ",""))
        return valid or ["09:00-12:00","12:00-15:00","15:00-18:00"]
    TIMEWINDOWS=_parse_timewindows(TIMEWINDOWS_RAW)

    def _dates_list(days:int)->List[str]:
        if days<=0: days=3
        base=time.time()
        return [time.strftime("%Y-%m-%d", time.localtime(base+i*86400)) for i in range(days)]

    async def _autobook_try_create(chat_id:int, data:Dict[str,Any])->Tuple[bool,str]:
        if not si:
            return False, "supply_integration –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"
        candidates=[
            ("create_autosupply_draft", ("chat_id","sku","qty","warehouse_id","warehouse_name","date","timeslot","auto_watch")),
            ("create_supply_draft", ("chat_id","sku","qty","warehouse_id","warehouse_name","date","timeslot","auto_watch")),
            ("enqueue_supply_task", ("chat_id","sku","qty","warehouse_id","warehouse_name","date","timeslot","auto_watch")),
            ("create_autosupply_request", ("chat_id","sku","qty","warehouse_id","warehouse_name","date","timeslot","auto_watch")),
        ]
        log.info("[AB] try create payload=%r", data)
        for fname,args in candidates:
            if hasattr(si,fname):
                fn=getattr(si,fname)
                log.info("[AB] using function %s", fname)
                try:
                    kwargs={k:(chat_id if k=="chat_id" else data.get(k)) for k in args}
                    res=await fn(**kwargs) if asyncio.iscoroutinefunction(fn) else fn(**kwargs)
                    log.info("[AB] %s OK result=%r", fname, res)
                    return True, f"{fname} OK: {res!r}"
                except Exception as e:
                    log.warning("[AB] %s error: %s", fname, e, exc_info=True)
                    return False, f"{fname} error: {e}"
        log.warning("[AB] no candidate function found")
        return False, "–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ –≤ supply_integration"

    def _ab_sku_page(page:int=0, page_size:int=10)->InlineKeyboardMarkup:
        total=len(SKU_LIST)
        if total==0:
            return InlineKeyboardMarkup(inline_keyboard=[[InlineKeyboardButton(text="–ù–µ—Ç SKU", callback_data="ab_nop")]])
        pages=(total+page_size-1)//page_size
        page=max(0,min(page,pages-1))
        start=page*page_size; end=min(start+page_size,total)
        rows=[]
        for sku in SKU_LIST[start:end]:
            nm=SKU_NAME_CACHE.get(sku,f"SKU {sku}")
            rows.append([InlineKeyboardButton(text=f"{nm[:40]} (SKU {sku})", callback_data=f"ab_sku:{sku}")])
        nav=[]
        if page>0: nav.append(InlineKeyboardButton(text="¬´", callback_data=f"ab_page:{page-1}"))
        nav.append(InlineKeyboardButton(text=f"{page+1}/{pages}", callback_data="ab_nop"))
        if page<pages-1: nav.append(InlineKeyboardButton(text="¬ª", callback_data=f"ab_page:{page+1}"))
        rows.append(nav)
        rows.append([InlineKeyboardButton(text="–û—Ç–º–µ–Ω–∞", callback_data="ab_cancel")])
        return InlineKeyboardMarkup(inline_keyboard=rows)

    @dp.callback_query(F.data=="menu_autobook")
    async def ab_menu(cb:CallbackQuery, state:FSMContext):
        ensure_admin(cb.from_user.id)
        if not SKU_LIST:
            await cb.message.answer("SKU_LIST –ø—É—Å—Ç. –ó–∞–¥–∞–π—Ç–µ –≤ .env.")
            await cb.answer(); return
        to_fetch=skus_needing_names()
        if to_fetch:
            prev=len(SKU_NAME_CACHE); mp,_=await ozon_product_names_by_sku(to_fetch)
            SKU_NAME_CACHE.update(mp); save_cache_if_needed(prev)
        await state.set_state(AutobookStates.picking_sku)
        await cb.message.answer("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–æ–≤–∞—Ä:", reply_markup=_ab_sku_page(0))
        await cb.answer()

    @dp.callback_query(F.data.startswith("ab_page:"))
    async def ab_page(cb:CallbackQuery, state:FSMContext):
        if await state.get_state()!=AutobookStates.picking_sku.state:
            await cb.answer(); return
        try: page=int(cb.data.split(":")[1])
        except Exception: page=0
        try:
            await cb.message.edit_reply_markup(reply_markup=_ab_sku_page(page))
        except Exception:
            await cb.message.answer("–°—Ç—Ä–∞–Ω–∏—Ü–∞:", reply_markup=_ab_sku_page(page))
        await cb.answer()

    @dp.callback_query(F.data=="ab_nop")
    async def ab_nop(cb:CallbackQuery):
        await cb.answer()

    @dp.callback_query(F.data=="ab_cancel")
    async def ab_cancel(cb:CallbackQuery, state:FSMContext):
        await state.clear()
        await cb.message.answer("–ê–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ.")
        await cb.answer()

    @dp.callback_query(F.data.startswith("ab_sku:"))
    async def ab_choose_sku(cb:CallbackQuery, state:FSMContext):
        if await state.get_state()!=AutobookStates.picking_sku.state:
            await cb.answer(); return
        try:
            sku=int(cb.data.split(":")[1])
        except Exception:
            await cb.answer("–û—à–∏–±–∫–∞ SKU"); return
        nm=SKU_NAME_CACHE.get(sku,f"SKU {sku}")
        log.info("[AB] SKU chosen sku=%s name=%s", sku, nm)
        await state.update_data(sku=sku, sku_name=nm)
        await state.set_state(AutobookStates.enter_qty)
        await cb.message.answer(f"–í—ã–±—Ä–∞–Ω–æ: {html.escape(nm)} (SKU {sku}). –í–≤–µ–¥–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ (—Ü–µ–ª–æ–µ —á–∏—Å–ª–æ).")
        await cb.answer()

    @dp.message(AutobookStates.enter_qty, F.text.regexp(r'^\d{1,9}$'))
    async def ab_qty_ok(m:Message, state:FSMContext):
        qty=int(m.text.strip())
        await state.update_data(qty=qty)
        if DEFAULT_DROPOFF_ID:
            wh_name=DEFAULT_DROPOFF_NAME or f"Dropoff {DEFAULT_DROPOFF_ID}"
            await state.update_data(warehouse_id=str(DEFAULT_DROPOFF_ID), warehouse_name=wh_name)
            await state.set_state(AutobookStates.choose_date)
            dates=_dates_list(DAYS_ENV)
            rows=[[InlineKeyboardButton(text=d, callback_data=f"ab_date:{d}")] for d in dates]
            rows.append([InlineKeyboardButton(text="–û—Ç–º–µ–Ω–∞", callback_data="ab_cancel")])
            await m.answer(f"–°–∫–ª–∞–¥: {html.escape(wh_name)}\n–í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç—É:", reply_markup=InlineKeyboardMarkup(inline_keyboard=rows))
            return
        rows, err=await ozon_stock_fbo(SKU_LIST)
        if err:
            await m.answer(f"–û—à–∏–±–∫–∞ Ozon API: {html.escape(err)}"); return
        agg=aggregate_rows(rows)
        wh_map={}
        for wmap in agg.values():
            for wkey,info in wmap.items():
                wh_map.setdefault(wkey, info["warehouse_name"])
        if not wh_map:
            await m.answer("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–∫–ª–∞–¥–æ–≤.")
            return
        options=sorted([(wk,nm) for wk,nm in wh_map.items()], key=lambda x:x[1].lower())
        await state.update_data(_wh_options=options)
        await state.set_state(AutobookStates.choose_warehouse)
        kb=[[InlineKeyboardButton(text=nm[:48], callback_data=f"ab_wh:{i}")] for i,(wk,nm) in enumerate(options)]
        kb.append([InlineKeyboardButton(text="–û—Ç–º–µ–Ω–∞", callback_data="ab_cancel")])
        await m.answer("–í—ã–±–µ—Ä–∏—Ç–µ —Å–∫–ª–∞–¥:", reply_markup=InlineKeyboardMarkup(inline_keyboard=kb))

    @dp.message(AutobookStates.enter_qty)
    async def ab_qty_bad(m:Message, state:FSMContext):
        await m.answer("–í–≤–µ–¥–∏—Ç–µ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ –∏–ª–∏ /cancel.")

    @dp.callback_query(F.data.startswith("ab_wh:"))
    async def ab_choose_wh(cb:CallbackQuery, state:FSMContext):
        if await state.get_state()!=AutobookStates.choose_warehouse.state:
            await cb.answer(); return
        data=await state.get_data()
        opts=data.get("_wh_options") or []
        try:
            idx=int(cb.data.split(":")[1]); wkey,wname=opts[idx]
        except Exception:
            await cb.answer("–û—à–∏–±–∫–∞ —Å–∫–ª–∞–¥–∞"); return
        await state.update_data(warehouse_id=wkey, warehouse_name=wname)
        await state.set_state(AutobookStates.choose_date)
        dates=_dates_list(DAYS_ENV)
        rows=[[InlineKeyboardButton(text=d, callback_data=f"ab_date:{d}")] for d in dates]
        rows.append([InlineKeyboardButton(text="–û—Ç–º–µ–Ω–∞", callback_data="ab_cancel")])
        await cb.message.answer(f"–°–∫–ª–∞–¥: {html.escape(wname)}\n–í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç—É –ø–æ—Å—Ç–∞–≤–∫–∏:", reply_markup=InlineKeyboardMarkup(inline_keyboard=rows))
        await cb.answer()

    @dp.callback_query(F.data.startswith("ab_date:"))
    async def ab_choose_date(cb:CallbackQuery, state:FSMContext):
        if await state.get_state()!=AutobookStates.choose_date.state:
            await cb.answer(); return
        date=cb.data.split(":",1)[1]
        await state.update_data(date=date)
        await state.set_state(AutobookStates.choose_timeslot)
        rows=[[InlineKeyboardButton(text=tw, callback_data=f"ab_ts:{i}")] for i,tw in enumerate(TIMEWINDOWS)]
        rows.append([InlineKeyboardButton(text="–ê–≤—Ç–æ–ø–æ–∏—Å–∫ —Ç–∞–π–º—Å–ª–æ—Ç–∞", callback_data="ab_ts:auto")])
        rows.append([InlineKeyboardButton(text="–û—Ç–º–µ–Ω–∞", callback_data="ab_cancel")])
        warn=" (—Ç–∞–π–º—Å–ª–æ—Ç—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã)" if DISABLE_TS_FALLBACK else ""
        await cb.message.answer(f"–î–∞—Ç–∞: {date}{warn}\n–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∞–π–º—Å–ª–æ—Ç –∏–ª–∏ –∞–≤—Ç–æ–ø–æ–∏—Å–∫:", reply_markup=InlineKeyboardMarkup(inline_keyboard=rows))
        await cb.answer()

    @dp.callback_query(F.data.startswith("ab_ts:"))
    async def ab_choose_ts(cb:CallbackQuery, state:FSMContext):
        if await state.get_state()!=AutobookStates.choose_timeslot.state:
            await cb.answer(); return
        token=cb.data.split(":",1)[1]
        auto_watch=False; timeslot=None
        if token=="auto":
            auto_watch=True
        else:
            try:
                idx=int(token); timeslot=TIMEWINDOWS[idx]
            except Exception:
                await cb.answer("–û—à–∏–±–∫–∞ —Å–ª–æ—Ç–∞"); return
        await state.update_data(timeslot=timeslot, auto_watch=auto_watch)
        data=await state.get_data()
        lines=[
            "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞—è–≤–∫–∏:",
            f"–¢–æ–≤–∞—Ä: {html.escape(data.get('sku_name',''))} (SKU {data.get('sku')})",
            f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {data.get('qty')}",
            f"–°–∫–ª–∞–¥: {html.escape(data.get('warehouse_name',''))}",
            f"–î–∞—Ç–∞: {data.get('date')}",
            f"–¢–∞–π–º—Å–ª–æ—Ç: {timeslot if timeslot else ('–ê–≤—Ç–æ–ø–æ–∏—Å–∫' if auto_watch else '-')}",
        ]
        rows=[[InlineKeyboardButton(text="–ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å", callback_data="ab_ok")],
              [InlineKeyboardButton(text="–û—Ç–º–µ–Ω–∞", callback_data="ab_cancel")]]
        await state.set_state(AutobookStates.confirm)
        await cb.message.answer("\n".join(lines), reply_markup=InlineKeyboardMarkup(inline_keyboard=rows))
        await cb.answer()

    @dp.callback_query(F.data=="ab_ok")
    async def ab_confirm(cb:CallbackQuery, state:FSMContext):
        if await state.get_state()!=AutobookStates.confirm.state:
            await cb.answer(); return
        data=await state.get_data()
        payload={
            "sku": int(data.get("sku")),
            "qty": int(data.get("qty")),
            "warehouse_id": str(data.get("warehouse_id")),
            "warehouse_name": data.get("warehouse_name"),
            "date": data.get("date"),
            "timeslot": data.get("timeslot"),
            "auto_watch": bool(data.get("auto_watch")),
        }
        log.info("[AB] confirm payload=%r", payload)
        await state.set_state(AutobookStates.creating)
        await cb.message.answer("–°–æ–∑–¥–∞—é –∑–∞—è–≤–∫—É...")
        ok,msg=await _autobook_try_create(cb.message.chat.id, payload)
        status="–°–æ–∑–¥–∞–Ω–æ" if ok else "–û—à–∏–±–∫–∞"
        _supply_log_append(cb.message.chat.id, {"ts":int(time.time()),"type":"text","text":f"[Fallback Autobook] {status}: {msg}\n{payload}"})
        if ok:
            await cb.message.answer("\n".join([
                "–ì–æ—Ç–æ–≤–æ.",
                f"–ó–∞—è–≤–∫–∞: SKU {payload['sku']} x {payload['qty']}",
                f"–°–∫–ª–∞–¥: {html.escape(payload['warehouse_name'])}",
                f"–î–∞—Ç–∞: {payload['date']} | –°–ª–æ—Ç: {payload['timeslot'] or '–ê–≤—Ç–æ–ø–æ–∏—Å–∫'}",
                "–ü–µ—Ä–µ–¥–∞–Ω–æ –≤ –æ–±—Ä–∞–±–æ—Ç–∫—É."
            ]))
        else:
            await cb.message.answer("\n".join([
                "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —á–µ—Ä–µ–∑ supply_integration.",
                f"–î–µ—Ç–∞–ª–∏: {msg}",
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ /autobook_diag."
            ]))
        log.info("[AB] finish ok=%s msg=%s", ok, msg)
        await state.clear()
        await cb.answer()

    @dp.message(Command("autobook_diag"))
    async def cmd_autobook_diag(m:Message):
        ensure_admin(m.from_user.id)
        funcs=[]
        if si:
            funcs=[fn for fn in ["create_autosupply_draft","create_supply_draft","enqueue_supply_task","create_autosupply_request"] if hasattr(si, fn)]
        need=skus_needing_names()
        lines=[
            "¬ß¬ßB¬ß¬ß–ê–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (fallback) –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞¬ß¬ßEB¬ß¬ß",
            f"DEFAULT_DROPOFF_ID={DEFAULT_DROPOFF_ID or '(none)'}",
            f"DEFAULT_DROPOFF_NAME={DEFAULT_DROPOFF_NAME or '(none)'}",
            f"TIMEWINDOWS={';'.join(TIMEWINDOWS)}",
            f"DAYS={DAYS_ENV} DISABLE_TS_FALLBACK={DISABLE_TS_FALLBACK}",
            f"Functions supply_integration={funcs or '(none found)'}",
            f"SKU_LIST size={len(SKU_LIST)}",
            f"Names known={len(SKU_LIST)-len(need)}/{len(SKU_LIST)} missing={len(need)}"
        ]
        await m.answer(build_html(lines), parse_mode="HTML")

# ===== Passthrough (–≤–Ω–µ—à–Ω–∏–π —Ñ–ª–æ—É) =====
if AUTOBOOK_ENABLED:
    @dp.message(StateFilter(abf.AB.enter_qty), F.text.regexp(r'^\s*\d{1,9}\s*$'))
    async def _ab_qty_ok_passthrough(m: Message, state: FSMContext):
        try:
            await abf.ab_qty_ok(m, state)
        except Exception as e:
            log.warning("Autobook passthrough qty_ok error: %s", e)

    @dp.message(StateFilter(abf.AB.enter_qty))
    async def _ab_qty_bad_passthrough(m: Message, state: FSMContext):
        try:
            await abf.ab_qty_bad(m, state)
        except Exception as e:
            log.warning("Autobook passthrough qty_bad error: %s", e)

# ===== Cluster & FACT Index =====
def resolve_cluster_for_warehouse(wkey:str, wname:str)->str:
    if CLUSTER_MAP:
        raw_id=None if wkey.startswith("name:") else wkey
        if raw_id and raw_id in CLUSTER_MAP: return CLUSTER_MAP[raw_id]
        if wname in CLUSTER_MAP: return CLUSTER_MAP[wname]
        return "–ü—Ä–æ—á–∏–µ"
    lname=(wname or "").lower()
    for cname, pats in CLUSTER_PATTERN_MAP.items():
        for p in pats:
            if p.search(lname):
                return cname
    return "–ü—Ä–æ—á–∏–µ"

def aggregate_clusters_from_fact(sku_section:Dict[int,Any])->Dict[str,Any]:
    clusters={}
    for sku,data in sku_section.items():
        for w in data.get("warehouses", []):
            cname=resolve_cluster_for_warehouse(w["wkey"], w["name"])
            c=clusters.setdefault(cname,{
                "name":cname,"total_qty":0,"total_need":0,"deficit_need":0,"sku_set":set(),
                "critical_sku":0,"mid_sku":0,"ok_sku":0,"warehouses":set()
            })
            qty=w["qty"]; gap_target=max(0, w["target"]-w["qty"])
            c["total_qty"]+=qty; c["total_need"]+=gap_target; c["deficit_need"]+=w["need"]
            c["warehouses"].add(w["name"])
            if sku not in c["sku_set"]: c["sku_set"].add(sku)
            cov=w["coverage"]
            if cov<0.5: c["critical_sku"]+=1
            elif cov<0.8: c["mid_sku"]+=1
            else: c["ok_sku"]+=1
    out={}
    for cname, meta in clusters.items():
        out[cname]={
            "name":cname,"total_qty":meta["total_qty"],"total_need":meta["total_need"],
            "deficit_need":meta["deficit_need"],"total_sku":len(meta["sku_set"]),
            "critical_sku":meta["critical_sku"],"mid_sku":meta["mid_sku"],
            "ok_sku":meta["ok_sku"],"warehouses":sorted(meta["warehouses"])
        }
    return out

def fancy_ratio_bar(parts:List[Tuple[int,str]], total:int, length:int=24)->str:
    if total<=0: return EMPTY_SEG*length
    raw=[]
    for count,color in parts:
        frac=count/total if total>0 else 0.0
        raw_blocks=frac*length
        raw.append((color,raw_blocks))
    allocated=[]; acc=0
    for color, raw_blocks in raw:
        blk=int(raw_blocks)
        allocated.append([color,blk,raw_blocks-blk]); acc+=blk
    remain=length-acc
    if remain>0:
        allocated.sort(key=lambda x:x[2], reverse=True)
        for i in range(remain):
            allocated[i][1]+=1
    bar="".join(color*blocks for color,blocks,_ in allocated)
    if len(bar)<length: bar+=EMPTY_SEG*(length-len(bar))
    return bar

def small_cov_bar(cov:float, length:int=12)->str:
    cov=max(0.0,min(1.0,cov))
    if cov<0.25: color=GR_FILL["red"]
    elif cov<0.5: color=GR_FILL["orange"]
    elif cov<0.8: color=GR_FILL["yellow"]
    else: color=GR_FILL["green"]
    filled=max(1,round(cov*length))
    return color*filled+EMPTY_SEG*(length-filled)

def format_pct_s(part:int,total:int)->str:
    if total<=0: return "0%"
    return f"{(part/total*100):.0f}%"

def build_cluster_detail(name:str, cluster_section:Dict[str,Any], sku_section:Dict[int,Any])->str:
    cl=cluster_section.get(name)
    if not cl:
        return build_html([f"{EMOJI_CLUSTER} –ö–ª–∞—Å—Ç–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω."])
    sku_rows=[]
    all_cov=[]
    cov_cat={"crit":0,"mid":0,"ok":0}
    for sku, skud in sku_section.items():
        worst_cov=1.0
        in_cluster=False
        need_cluster=0
        for w in skud.get("warehouses", []):
            if resolve_cluster_for_warehouse(w["wkey"], w["name"])==name:
                in_cluster=True
                need_cluster+=w["need"]
                worst_cov=min(worst_cov, w["coverage"])
        if in_cluster:
            all_cov.append(worst_cov)
            if worst_cov<0.5: cov_cat["crit"]+=1
            elif worst_cov<0.8: cov_cat["mid"]+=1
            else: cov_cat["ok"]+=1
        if need_cluster>0:
            sku_rows.append((sku, skud["name"], need_cluster, worst_cov))
    sku_rows.sort(key=lambda x:(-x[2], x[3]))
    total_present=len(all_cov)
    cluster_worst=min(all_cov) if all_cov else 0.0
    cluster_avg=sum(all_cov)/total_present if total_present else 0.0
    sev= "üî• –ö–†–ò–¢–ò–ß–ù–û" if cluster_worst<0.25 else "‚ö† –ù–ò–ñ–ï –ù–û–†–ú–´" if cluster_worst<0.5 else "üü® –†–ò–°–ö" if cluster_worst<0.8 else "üü© OK"
    dist_bar=fancy_ratio_bar([(cov_cat["crit"],"üü•"),(cov_cat["mid"],"üü®"),(cov_cat["ok"],"üü©")], total_present, 24)
    worst_bar=small_cov_bar(cluster_worst,20)
    avg_bar=small_cov_bar(cluster_avg,20)
    lines=[
        f"üó∫‚ú® ¬ß¬ßB¬ß¬ß–ö–õ–ê–°–¢–ï–†: {html.escape(name)}¬ß¬ßEB¬ß¬ß",
        SEP_THIN,
        f"–°—Ç–∞—Ç—É—Å: {sev}",
        f"–°–∫–ª–∞–¥—ã: {len(cl['warehouses'])} | SKU –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ: {cl['total_sku']} | –£—á–∏—Ç—ã–≤–∞–ª–æ—Å—å(SKU coverage): {total_present}",
        f"Qty: {cl['total_qty']}  TotalNeed: {cl['total_need']}  DeficitNeed: {cl['deficit_need']}",
        "",
        "–ü–æ–∫—Ä—ã—Ç–∏–µ —Ö—É–¥—à–µ–µ –ø–æ SKU:",
        f"  –•—É–¥—à–µ–µ: {worst_bar} {int(cluster_worst*100):02d}%",
        f"  –°—Ä–µ–¥–Ω–µ–µ: {avg_bar} {int(cluster_avg*100):02d}%",
        "",
        "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (worst coverage):",
        f"  üü• <50%: {cov_cat['crit']} ({format_pct_s(cov_cat['crit'], total_present)})  "
        f"üü® 50‚Äì80%: {cov_cat['mid']} ({format_pct_s(cov_cat['mid'], total_present)})  "
        f"üü© ‚â•80%: {cov_cat['ok']} ({format_pct_s(cov_cat['ok'], total_present)})",
        f"  BAR: {dist_bar}",
        SEP_THIN,
        f"–°–∫–ª–∞–¥—ã ({len(cl['warehouses'])}): {', '.join(cl['warehouses']) if cl['warehouses'] else '-'}",
        SEP_THIN
    ]
    if sku_rows:
        lines.append("¬ß¬ßB¬ß¬ßTOP –¥–µ—Ñ–∏—Ü–∏—Ç–Ω—ã—Ö SKU¬ß¬ßEB¬ß¬ß")
        header=f"{'#':>2} {'SKU':>8} {'Need':>8} {'Cov':>5}  Bar"
        lines.append(header); lines.append("-"*len(header))
        for i,(sku,nm,need,cov) in enumerate(sku_rows[:30],1):
            bar=small_cov_bar(cov,12)
            lines.append(f"{i:>2} {sku:>8} {need:>8} {int(cov*100):>3d}%  {bar}  {nm[:45]}")
    else:
        lines.append("–ù–µ—Ç –¥–µ—Ñ–∏—Ü–∏—Ç–Ω—ã—Ö SKU.")
    lines.extend([SEP_THIN,"Need ‚Äì –Ω–µ–¥–æ–±–æ—Ä –¥–æ target. DeficitNeed ‚Äì –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å (<norm).",LEGEND_TEXT,SEP_THIN])
    return build_html(lines)

def build_fact_index(rows:List[dict], flat:List[dict], ccache:Dict[Tuple[int,str],Dict[str,Any]]):
    global FACT_INDEX
    agg=aggregate_rows(rows)
    sku_section={}
    wh_agg={}
    for sku,wmap in agg.items():
        name=SKU_NAME_CACHE.get(sku,f"SKU {sku}")
        entry={"name":name,"total_qty":0,"total_need":0,"deficit_need":0,"worst_coverage":1.0,"warehouses":[]}
        for wkey,info in wmap.items():
            qty=info["qty"]
            st=evaluate_position_cached(sku,wkey,qty,ccache)
            norm=st["norm"] or 1
            coverage=qty/norm if norm else 0
            need_def=st["need"] if st["is_low"] else 0
            gap_target=max(0, st["target"]-qty)
            entry["total_qty"]+=qty; entry["deficit_need"]+=need_def; entry["total_need"]+=gap_target
            entry["worst_coverage"]=min(entry["worst_coverage"], coverage)
            entry["warehouses"].append({
                "wkey":wkey,"name":info["warehouse_name"],"qty":qty,"norm":st["norm"],"target":st["target"],
                "need":need_def,"coverage":round(coverage,4),"history_used":st["history_used"]
            })
            wm=wh_agg.setdefault(wkey,{"name":info["warehouse_name"],"total_qty":0,"total_need":0,"deficit_need":0,
                                       "sku_set":set(),"critical_sku":0,"mid_sku":0,"ok_sku":0})
            wm["total_qty"]+=qty; wm["total_need"]+=gap_target; wm["deficit_need"]+=need_def
            if sku not in wm["sku_set"]: wm["sku_set"].add(sku)
            if coverage<0.5: wm["critical_sku"]+=1
            elif coverage<0.8: wm["mid_sku"]+=1
            else: wm["ok_sku"]+=1
        entry["warehouses"].sort(key=lambda x:x["coverage"])
        sku_section[sku]=entry
    top_deficits=sorted(
        ({"sku":s,"name":v["name"],"coverage":round(v["worst_coverage"],4),"deficit_need":v["deficit_need"]}
         for s,v in sku_section.items()),
        key=lambda x:x["coverage"]
    )[:LLM_TOP_DEFICITS]
    wh_section={}
    for k, meta in wh_agg.items():
        wh_section[k]={
            "name":meta["name"],"total_qty":meta["total_qty"],
            "total_need":meta["total_need"],"deficit_need":meta["deficit_need"],
            "total_sku":len(meta["sku_set"]),
            "critical_sku":meta["critical_sku"],"mid_sku":meta["mid_sku"],"ok_sku":meta["ok_sku"]
        }
    cluster_section=aggregate_clusters_from_fact(sku_section)
    top_clusters=sorted(
        ({"cluster":c,"name":v["name"],"total_need":v["total_need"],"deficit_need":v["deficit_need"]}
         for c,v in cluster_section.items()),
        key=lambda x:x["total_need"], reverse=True
    )[:LLM_TOP_CLUSTERS]
    top_warehouses=sorted(
        ({"wkey":k,"name":v["name"],"total_need":v["total_need"],"deficit_need":v["deficit_need"]}
         for k,v in wh_section.items()),
        key=lambda x:x["total_need"], reverse=True
    )[:LLM_TOP_WAREHOUSES]
    sample=[f"{sku}:{sku_section[sku]['name'][:50]}" for sku in sorted(sku_section.keys())[:LLM_INVENTORY_SAMPLE_SKU]]
    FACT_INDEX={
        "updated_ts":int(time.time()),
        "snapshot_ts":LAST_SNAPSHOT_TS,
        "sku":sku_section,
        "warehouse":wh_section,
        "cluster":cluster_section,
        "top_deficits":top_deficits,
        "top_warehouses":top_warehouses,
        "top_clusters":top_clusters,
        "inventory_overview":{"total_sku":len(sku_section),"sample_skus":sample}
    }

# ===== FACT context & AI =====
FULL_DUMP_PATTERNS=[
    "–≤–µ—Å—å –æ–±—ä–µ–º","–≤–µ—Å—å –æ–±—ä—ë–º","–≤—Å–µ –¥–∞–Ω–Ω—ã–µ","–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫","–ø–æ–ª–Ω—ã–π –ø–µ—Ä–µ—á–µ–Ω—å",
    "full dump","–≤—Å–µ sku","–≤–µ—Å—å —Å–ø–∏—Å–æ–∫","–≤–µ—Å—å –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç","–¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã","–≤—Å–µ —Ç–æ–≤–∞—Ä—ã"
]
PRODUCT_LIST_PATTERNS=[
    "–∫–∞–∫–∏–µ —Ç–æ–≤–∞—Ä—ã","—Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤","–ø–µ—Ä–µ—á–µ–Ω—å —Ç–æ–≤–∞—Ä–æ–≤","–∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç","–∫–∞–∫–∏–µ —É –Ω–∞—Å —Ç–æ–≤–∞—Ä—ã","—á—Ç–æ –∑–∞ —Ç–æ–≤–∞—Ä—ã"
]
def extract_skus_from_question(q:str)->List[int]:
    return [int(m.group()) for m in re.finditer(r"\b\d{3,}\b", q)]
def is_full_dump_question(q:str)->bool:
    ql=q.lower()
    return any(p in ql for p in FULL_DUMP_PATTERNS)
def is_list_products_question(q:str)->bool:
    ql=q.lower()
    return any(p in ql for p in PRODUCT_LIST_PATTERNS)

def _trim_facts(text:str)->str:
    if len(text)<=LLM_FACT_SOFT_LIMIT_CHARS: return text
    lines=text.splitlines(); keep=[]; n=0
    for ln in lines:
        if n+len(ln)+1>int(LLM_FACT_SOFT_LIMIT_CHARS*0.95):
            keep.append("... (—É—Å–µ—á–µ–Ω–æ)"); break
        keep.append(ln); n+=len(ln)+1
    return "\n".join(keep)

def build_facts_block(question:str)->Tuple[str,str]:
    if not FACT_INDEX: return "NO_DATA_INDEX","empty"
    q=question.strip(); skus_in=extract_skus_from_question(q)
    sku_data=FACT_INDEX.get("sku",{}); inv=FACT_INDEX.get("inventory_overview",{})
    mode="general"
    if is_full_dump_question(q):
        mode="full_dump"
        lines=[f"updated_ts={FACT_INDEX['updated_ts']} snapshot_ts={FACT_INDEX['snapshot_ts']} MODE=FULL_DUMP",
               f"TOTAL_SKU={inv.get('total_sku')}"]
        for sku, entry in list(sku_data.items())[:LLM_FULL_DETAIL_SKU]:
            lines.append(f"SKU {sku} '{entry['name']}' worst_cov={round(entry['worst_coverage'],3)} total_qty={entry['total_qty']} deficit_need={entry['deficit_need']}")
            for w in entry["warehouses"][:LLM_FULL_DETAIL_WAREHOUSES]:
                lines.append(f"  WH '{w['name']}' qty={w['qty']} norm={w['norm']} target={w['target']} need={w['need']} cov={w['coverage']} hist={1 if w['history_used'] else 0}")
        lines.append("TOP_DEFICITS:")
        for td in FACT_INDEX["top_deficits"]:
            lines.append(f"  SKU {td['sku']} '{td['name']}' cov={td['coverage']} deficit_need={td['deficit_need']}")
        lines.append("TOP_WAREHOUSES:")
        for w in FACT_INDEX["top_warehouses"]:
            lines.append(f"  WH {w['wkey']} '{w['name']}' total_need={w['total_need']} deficit_need={w['deficit_need']}")
        if FACT_INDEX.get("top_clusters"):
            lines.append("TOP_CLUSTERS:")
            for c in FACT_INDEX["top_clusters"]:
                lines.append(f"  CL {c['cluster']} '{c['name']}' total_need={c['total_need']} deficit_need={c['deficit_need']}")
        return _trim_facts("\n".join(lines)), mode
    if skus_in:
        mode="specific"
        lines=[f"updated_ts={FACT_INDEX['updated_ts']} snapshot_ts={FACT_INDEX['snapshot_ts']} MODE=SPECIFIC_SKU"]
        for sku in skus_in[:LLM_MAX_CONTEXT_SKU]:
            entry=sku_data.get(sku)
            if not entry:
                lines.append(f"SKU {sku}: NO_DATA"); continue
            lines.append(f"SKU {sku} '{entry['name']}' worst_cov={round(entry['worst_coverage'],3)} total_qty={entry['total_qty']} deficit_need={entry['deficit_need']}")
            for w in entry["warehouses"][:LLM_MAX_CONTEXT_WAREHOUSE]:
                lines.append(f"  WH '{w['name']}' qty={w['qty']} norm={w['norm']} target={w['target']} need={w['need']} cov={w['coverage']}")
        return _trim_facts("\n".join(lines)), mode
    if is_list_products_question(q):
        mode="list"
        lines=[f"updated_ts={FACT_INDEX['updated_ts']} snapshot_ts={FACT_INDEX['snapshot_ts']} MODE=LIST_PRODUCTS",
               f"TOTAL_SKU={inv.get('total_sku')}",
               "SAMPLE_SKUS:"]
        for s in inv.get("sample_skus",[])[:LLM_INVENTORY_SAMPLE_SKU]:
            lines.append(f"  {s}")
        lines.append("TOP_DEFICITS:")
        for td in FACT_INDEX["top_deficits"][:LLM_MAX_CONTEXT_SKU]:
            lines.append(f"  SKU {td['sku']} '{td['name']}' cov={td['coverage']} deficit_need={td['deficit_need']}")
        if FACT_INDEX.get("top_clusters"):
            lines.append("TOP_CLUSTERS:")
            for c in FACT_INDEX["top_clusters"][:LLM_TOP_CLUSTERS]:
                lines.append(f"  CL {c['cluster']} '{c['name']}' total_need={c['total_need']} deficit_need={c['deficit_need']}")
        return _trim_facts("\n".join(lines)), mode
    mode="general"
    lines=[f"updated_ts={FACT_INDEX['updated_ts']} snapshot_ts={FACT_INDEX['snapshot_ts']} MODE=GENERAL",
           f"TOTAL_SKU={inv.get('total_sku')}",
           "TOP_DEFICITS:"]
    for td in FACT_INDEX["top_deficits"][:LLM_MAX_CONTEXT_SKU]:
        lines.append(f"  SKU {td['sku']} '{td['name']}' cov={td['coverage']} deficit_need={td['deficit_need']}")
    lines.append("TOP_WAREHOUSES:")
    for w in FACT_INDEX["top_warehouses"][:LLM_MAX_CONTEXT_WAREHOUSE]:
        lines.append(f"  WH {w['wkey']} '{w['name']}' total_need={w['total_need']} deficit_need={w['deficit_need']}")
    if FACT_INDEX.get("top_clusters"):
        lines.append("TOP_CLUSTERS:")
        for c in FACT_INDEX["top_clusters"][:LLM_TOP_CLUSTERS]:
            lines.append(f"  CL {c['cluster']} '{c['name']}' total_need={c['total_need']} deficit_need={c['deficit_need']}")
    return _trim_facts("\n".join(lines)), mode

def cache_answer(question:str, mode:str, answer:str):
    if not LLM_ENABLE_ANSWER_CACHE or not FACT_INDEX: return
    key=f"{mode}|{FACT_INDEX.get('snapshot_ts')}|{question.strip().lower()}"
    ANSWER_CACHE[hashlib.sha1(key.encode()).hexdigest()]=answer

def get_cached_answer(question:str, mode:str)->Optional[str]:
    if not LLM_ENABLE_ANSWER_CACHE or not FACT_INDEX: return None
    key=f"{mode}|{FACT_INDEX.get('snapshot_ts')}|{question.strip().lower()}"
    return ANSWER_CACHE.get(hashlib.sha1(key.encode()).hexdigest())

def build_messages_fact(question:str)->Tuple[List[Dict[str,str]], str]:
    facts, mode=build_facts_block(question)
    if facts=="NO_DATA_INDEX":
        return [
            {"role":"system","content":"–ù–µ—Ç –∏–Ω–¥–µ–∫—Å–∞ FACTS. –í—ã–ø–æ–ª–Ω–∏—Ç–µ /analyze."},
            {"role":"user","content":question}
        ], mode
    system=("–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ –æ—Å—Ç–∞—Ç–∫–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –¥–∞–Ω–Ω—ã–µ –∏–∑ FACTS; –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö'.")
    return [
        {"role":"system","content":system},
        {"role":"user","content":f"–í–æ–ø—Ä–æ—Å:\n{question}\n\nFACTS:\n{facts}"}
    ], mode

# ===== AI Styling + GigaChat =====
HIGHLIGHT_PATTERNS=[
    (r"\b(\d{1,3}(?:[\s.,]\d{3})+|\d+)\b","num"),
    (r"\b(–¥–µ—Ñ–∏—Ü–∏—Ç\w*)\b","kw"),
    (r"\b(—Å–∫–ª–∞–¥\w*)\b","kw"),
    (r"\b(–∫–ª–∞—Å—Ç\w*)\b","kw"),
    (r"\b(–Ω–æ—Ä–º[–∞–∏—ã]?)\b","kw"),
    (r"\b(—Ü–µ–ª—å|target)\b","kw"),
    (r"\b(–ø–æ–∫—Ä—ã—Ç–∏[–µ—è]|coverage)\b","kw"),
    (r"\b(SKU\s*\d+)\b","sku"),
    (r"\b\d{1,3}%\b","pct")
]

def style_ai_answer(question:str, raw:str, mode:str, fact_mode:bool)->str:
    if not BOT_STATE.get("style_enabled", True):
        return build_html([raw])
    lines=[l.rstrip() for l in raw.splitlines()]
    compact=[]; blank=False
    for l in lines:
        if not l.strip():
            if not blank: compact.append("")
            blank=True
        else:
            compact.append(l); blank=False
    def bulletize(s:str)->str:
        st=s.lstrip("-*‚Ä¢‚Äî ")
        if re.match(r"^\d+[\).]\s", st): return "‚Ññ "+st
        return "‚Ä¢ "+st
    processed=[]
    for l in compact:
        lt=l.strip()
        if not lt:
            processed.append(""); continue
        if re.match(r"^[-*‚Ä¢‚Äî]\s", lt) or re.match(r"^\d+[\).]\s", lt):
            processed.append(bulletize(lt))
        else:
            processed.append(lt)
    if fact_mode and not any("–∏—Ç–æ–≥" in x.lower() for x in processed[-6:]):
        tail=[x for x in processed[-10:] if re.search(r"\d", x)]
        if tail:
            processed.append(""); processed.append("–ò–¢–û–ì: " + "; ".join(tail[-3:]))
    styled=html.escape("\n".join(processed))
    def repl(m): return f"<b>{html.escape(m.group(1))}</b>"
    for pat,_ in HIGHLIGHT_PATTERNS:
        styled=re.sub(pat,repl,styled,flags=re.IGNORECASE)
    header=f"{EMOJI_AI} <b>–û—Ç–≤–µ—Ç</b> | —Ä–µ–∂–∏–º={'FACT' if fact_mode else 'GENERAL'}:{mode} | snap={FACT_INDEX.get('snapshot_ts','-')} | {time.strftime('%H:%M:%S')}"
    qline=f"<b>–í–æ–ø—Ä–æ—Å:</b> {html.escape(question)}"
    return f"{header}\n{SEP_THIN}\n{qline}\n{SEP_THIN}\n{styled}"

def _sanitize_cred(val:str)->str:
    return re.sub(r'[\r\n\t]','', val.strip().strip('"').strip("'")) if val else ""

def _gigachat_verify_param():
    if GIGACHAT_SSL_MODE=="insecure": return False
    if GIGACHAT_SSL_MODE=="custom":
        if not os.path.isfile(GIGACHAT_CA_CERT):
            log.warning("CA –Ω–µ –Ω–∞–π–¥–µ–Ω (%s)", GIGACHAT_CA_CERT)
            return GIGACHAT_VERIFY_SSL
        return GIGACHAT_CA_CERT
    return GIGACHAT_VERIFY_SSL

def _read_token_cache_file():
    if not GIGACHAT_TOKEN_CACHE_FILE.exists(): return None
    try: return json.loads(GIGACHAT_TOKEN_CACHE_FILE.read_text("utf-8"))
    except Exception: return None

def _write_token_cache_file(data:dict):
    try: _atomic_write(GIGACHAT_TOKEN_CACHE_FILE, json.dumps(data, ensure_ascii=False, indent=2))
    except Exception as e: log.warning("token cache write error: %s", e)

def _token_valid(tok:dict)->bool:
    if not tok: return False
    exp=tok.get("expires_epoch")
    if not exp and tok.get("obtained_at") and tok.get("expires_in"):
        exp=tok["obtained_at"]+int(tok["expires_in"])
    if not exp: return False
    return (exp-time.time())>120

async def get_gigachat_token(force=False)->str:
    if not GIGACHAT_ENABLED: raise RuntimeError("LLM_PROVIDER != gigachat")
    cid=_sanitize_cred(GIGACHAT_CLIENT_ID); sec=_sanitize_cred(GIGACHAT_CLIENT_SECRET)
    if not cid or not sec: raise RuntimeError("–ü—É—Å—Ç–æ–π CLIENT_ID –∏–ª–∏ CLIENT_SECRET.")
    global _GIGACHAT_TOKEN_MEM
    if not force and _token_valid(_GIGACHAT_TOKEN_MEM):
        return _GIGACHAT_TOKEN_MEM["access_token"]
    if not force:
        cache=_read_token_cache_file()
        if _token_valid(cache):
            _GIGACHAT_TOKEN_MEM=cache
            return cache["access_token"]
    raw_pair=f"{cid}:{sec}"
    basic=base64.b64encode(raw_pair.encode()).decode()
    headers={"Authorization":f"Basic {basic}","RqUID":str(uuid.uuid4()),
             "Content-Type":"application/x-www-form-urlencoded","Accept":"application/json"}
    data={"scope":GIGACHAT_SCOPE}
    try:
        async with httpx.AsyncClient(verify=_gigachat_verify_param(), timeout=GIGACHAT_TIMEOUT_SECONDS) as client:
            resp=await client.post(GIGACHAT_TOKEN_URL,data=data,headers=headers)
    except Exception as e:
        raise RuntimeError(f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ —Ç–æ–∫–µ–Ω–∞: {e}")
    if resp.status_code>=400:
        raise RuntimeError(f"HTTP {resp.status_code}: {resp.text}")
    js=resp.json()
    obtained=int(time.time())
    exp_epoch=None
    if isinstance(js.get("expires_at"),(int,float)): exp_epoch=int(js["expires_at"])
    if not exp_epoch and js.get("expires_in"):
        try: exp_epoch=obtained+int(js["expires_in"])
        except Exception: pass
    if not exp_epoch: exp_epoch=obtained+1800
    token_obj={"access_token":js.get("access_token"),"obtained_at":obtained,
               "expires_in":js.get("expires_in"),"expires_epoch":exp_epoch}
    if not token_obj["access_token"]:
        raise RuntimeError(f"–û—Ç–≤–µ—Ç –±–µ–∑ access_token: {js}")
    _GIGACHAT_TOKEN_MEM=token_obj; _write_token_cache_file(token_obj)
    return token_obj["access_token"]

async def llm_fact_answer(question:str)->Tuple[str,str]:
    if not GIGACHAT_ENABLED: return "LLM –æ—Ç–∫–ª—é—á—ë–Ω.","off"
    if not (_sanitize_cred(GIGACHAT_CLIENT_ID) and _sanitize_cred(GIGACHAT_CLIENT_SECRET)):
        return "–ù–µ—Ç –∫—Ä–µ–¥–æ–≤ GigaChat.","off"
    q=question.strip()
    if not q: return "–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å.","empty"
    global _LAST_AI_CALL
    now=time.time()
    if AI_MIN_INTERVAL_SECONDS>0 and (now-_LAST_AI_CALL)<AI_MIN_INTERVAL_SECONDS:
        return f"–°–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ. –ü–æ–¥–æ–∂–¥–∏—Ç–µ {AI_MIN_INTERVAL_SECONDS-int(now-_LAST_AI_CALL)} —Å–µ–∫.","rate"
    messages, mode=build_messages_fact(q)
    cached=get_cached_answer(q,mode)
    if cached: return "(cached)\n"+cached, mode
    _LAST_AI_CALL=now
    try: token=await get_gigachat_token()
    except Exception as e: return f"–û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∞: {e}", "token"
    payload={"model":GIGACHAT_MODEL,"messages":messages,"temperature":min(0.2,GIGACHAT_TEMPERATURE),"max_tokens":GIGACHAT_MAX_TOKENS}
    try:
        async with httpx.AsyncClient(verify=_gigachat_verify_param(), timeout=GIGACHAT_TIMEOUT_SECONDS) as client:
            r=await client.post(GIGACHAT_API_URL,json=payload,headers={"Authorization":f"Bearer {token}","Content-Type":"application/json"})
            if r.status_code==401:
                _GIGACHAT_TOKEN_MEM={}
                token=await get_gigachat_token(force=True)
                r=await client.post(GIGACHAT_API_URL,json=payload,headers={"Authorization":f"Bearer {token}","Content-Type":"application/json"})
            if r.status_code>=400:
                if r.status_code==400 and "decode" in r.text.lower():
                    return f"–û—à–∏–±–∫–∞ 400 (Authorization decode). –ü—Ä–æ–≤–µ—Ä—å CLIENT_ID/SECRET. –û—Ç–≤–µ—Ç: {r.text}", "auth"
                return f"GigaChat HTTP {r.status_code}: {r.text[:300]}", "http"
            data=r.json()
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}", "net"
    ch=data.get("choices")
    if not ch: return f"–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç: {data}","empty"
    text=(ch[0].get("message",{}).get("content") or "").strip()
    if not text: return f"–ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç: {data}","empty"
    cache_answer(q,mode,text)
    return text, mode

GENERAL_WORK_KEYWORDS=["sku","—Å–∫–ª–∞–¥","—Å–∫–ª–∞–¥—ã","–¥–µ—Ñ–∏—Ü–∏—Ç","–Ω–æ—Ä–º","target","–ø–æ–∫—Ä—ã—Ç","–æ—Å—Ç–∞—Ç","—Ç–æ–≤–∞—Ä","ozon","–æ–∑–æ–Ω","–∫–ª–∞—Å—Ç–µ—Ä"]
def looks_like_work_question(q:str)->bool:
    ql=q.lower()
    if re.search(r"\b\d{5,}\b", ql): return True
    return any(k in ql for k in GENERAL_WORK_KEYWORDS)

def add_general_history(chat_id:int, role:str, content:str):
    arr=GENERAL_HISTORY.setdefault(chat_id,[])
    arr.append({"role":role,"content":content})
    if len(arr)>GENERAL_HISTORY_MAX:
        del arr[0:len(arr)-GENERAL_HISTORY_MAX]

def build_general_messages(chat_id:int, question:str)->List[Dict[str,str]]:
    history=GENERAL_HISTORY.get(chat_id,[])
    sys=("–¢—ã –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫. –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ —Å–∫–ª–∞–¥—ã/–æ—Å—Ç–∞—Ç–∫–∏/SKU ‚Äî —É–ø–æ–º—è–Ω–∏ /ai.")
    msgs=[{"role":"system","content":sys}]
    for msg in history[-(GENERAL_HISTORY_MAX-1):]:
        msgs.append(msg)
    msgs.append({"role":"user","content":question})
    if looks_like_work_question(question):
        msgs.append({"role":"system","content":"–†–∞–±–æ—á–∏–π –≤–æ–ø—Ä–æ—Å ‚Äî –ø—Ä–µ–¥–ª–æ–∂–∏ /ai."})
    return msgs

async def llm_general_answer(chat_id:int, question:str)->Tuple[str,str]:
    if not GIGACHAT_ENABLED: return "LLM –æ—Ç–∫–ª—é—á—ë–Ω.","off"
    if not (_sanitize_cred(GIGACHAT_CLIENT_ID) and _sanitize_cred(GIGACHAT_CLIENT_SECRET)):
        return "–ù–µ—Ç –∫—Ä–µ–¥–æ–≤ GigaChat.","off"
    q=question.strip()
    if not q: return "–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å.","empty"
    global _LAST_AI_CALL
    now=time.time()
    if AI_MIN_INTERVAL_SECONDS>0 and (now-_LAST_AI_CALL)<AI_MIN_INTERVAL_SECONDS:
        return f"–°–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ. –ü–æ–¥–æ–∂–¥–∏—Ç–µ {AI_MIN_INTERVAL_SECONDS-int(now-_LAST_AI_CALL)} —Å–µ–∫.","rate"
    _LAST_AI_CALL=now
    messages=build_general_messages(chat_id,q)
    try: token=await get_gigachat_token()
    except Exception as e: return f"–û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∞: {e}", "token"
    payload={"model":GIGACHAT_MODEL,"messages":messages,"temperature":LLM_GENERAL_TEMPERATURE,"max_tokens":GIGACHAT_MAX_TOKENS}
    try:
        async with httpx.AsyncClient(verify=_gigachat_verify_param(), timeout=GIGACHAT_TIMEOUT_SECONDS) as client:
            r=await client.post(GIGACHAT_API_URL,json=payload,headers={"Authorization":f"Bearer {token}","Content-Type":"application/json"})
            if r.status_code>=400:
                if r.status_code==400 and "decode" in r.text.lower():
                    return f"–û—à–∏–±–∫–∞ 400 (Authorization decode). –ü—Ä–æ–≤–µ—Ä—å CLIENT_ID/SECRET. –û—Ç–≤–µ—Ç: {r.text}", "auth"
                return f"GigaChat HTTP {r.status_code}: {r.text[:300]}", "http"
            data=r.json()
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}","net"
    ch=data.get("choices")
    if not ch: return f"–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç: {data}","empty"
    text=(ch[0].get("message",{}).get("content") or "").strip()
    if not text: return f"–ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç: {data}","empty"
    add_general_history(chat_id,"user",q)
    add_general_history(chat_id,"assistant",text)
    return text,"general"

# ===== Messaging helpers =====
async def send_safe_message(chat_id:int, text:str, **kwargs):
    try:
        return await bot.send_message(chat_id, text, **kwargs)
    except TelegramRetryAfter as e:
        await asyncio.sleep(e.retry_after)
        return await bot.send_message(chat_id, text, **kwargs)
    except Exception as e:
        log.warning("send fail: %s", e)

async def send_long(chat_id:int, text:str, kb:Optional[InlineKeyboardMarkup]=None):
    max_len=3900
    parts=[]; buf=[]; ln=0
    for line in text.split("\n"):
        L=len(line)+1
        if buf and ln+L>max_len:
            parts.append("\n".join(buf)); buf=[line]; ln=L
        else:
            buf.append(line); ln+=L
    if buf: parts.append("\n".join(buf))
    for i,chunk in enumerate(parts):
        await send_safe_message(chat_id, chunk.rstrip() or "\u200B",
                                parse_mode="HTML",
                                disable_web_page_preview=True,
                                reply_markup=kb if (kb and i==len(parts)-1) else None)
        await asyncio.sleep(0.02)

# ===== Supply notifications =====
async def supply_notify_text(chat_id:int, text:str):
    _supply_log_append(chat_id, {"ts":int(time.time()),"type":"text","text":text})
    await send_safe_message(chat_id, text, parse_mode="HTML", disable_web_page_preview=True)

async def supply_notify_file(chat_id:int, file_path:str, caption:str=""):
    _supply_log_append(chat_id, {"ts":int(time.time()),"type":"file","file":file_path,"caption":caption})
    try:
        doc=FSInputFile(file_path); await bot.send_document(chat_id, document=doc, caption=caption)
    except Exception as e:
        log.warning("send_document fail: %s", e)
        await send_safe_message(chat_id, f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–∞–π–ª: {html.escape(str(e))}\n{caption}")

# ===== Analyze flow =====
async def handle_analyze(chat_id:int, verbose:bool=True):
    global LAST_ANALYZE_MS, LAST_ANALYZE_ERROR
    async with ANALYZE_LOCK:
        start=time.time(); LAST_ANALYZE_ERROR=None
        temp_msg=None
        try:
            if verbose: temp_msg=await send_safe_message(chat_id,"‚öô –ê–Ω–∞–ª–∏–∑...")
            need_snapshot=(time.time()-LAST_SNAPSHOT_TS>SNAPSHOT_STALE_MINUTES*60)
            rows, err=await ozon_stock_fbo(SKU_LIST)
            if err:
                LAST_ANALYZE_ERROR=err
                await send_safe_message(chat_id,f"–û—à–∏–±–∫–∞ Ozon API: {html.escape(err)}")
                if temp_msg:
                    try: await temp_msg.delete()
                    except Exception: pass
                return
            if need_snapshot and time.time()-LAST_SNAPSHOT_TS>SNAPSHOT_MIN_REUSE_SECONDS:
                append_snapshot(rows); await flush_history_if_needed(force=True)
            to_fetch=skus_needing_names()
            if to_fetch:
                prev=len(SKU_NAME_CACHE); mp,_=await ozon_product_names_by_sku(to_fetch)
                SKU_NAME_CACHE.update(mp); save_cache_if_needed(prev)
            ccache=build_consumption_cache()
            report, flat=generate_deficit_report(rows, SKU_NAME_CACHE, ccache)
            LAST_DEFICIT_CACHE[chat_id]={"flat":flat,"timestamp":int(time.time()),
                                         "report":report,"raw_rows":rows,"consumption_cache":ccache}
            try: build_fact_index(rows, flat, ccache)
            except Exception as e: log.warning("FACT_INDEX build error: %s", e)
            kb=deficit_filters_kb()
            if flat: await send_long(chat_id, report, kb=kb)
            else: await send_safe_message(chat_id, report)
            if temp_msg:
                try: await temp_msg.delete()
                except Exception: pass
        except Exception as e:
            LAST_ANALYZE_ERROR=str(e)
            log.exception("Analyze error")
            await send_safe_message(chat_id,f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {html.escape(str(e))}")
        finally:
            LAST_ANALYZE_MS=(time.time()-start)*1000
            await flush_history_if_needed()

# ===== SKU pagination =====
def build_stock_page(page:int)->InlineKeyboardMarkup:
    total=len(SKU_LIST)
    if total==0:
        return InlineKeyboardMarkup(inline_keyboard=[[InlineKeyboardButton(text="–ù–µ—Ç SKU", callback_data="noop")]])
    pages=(total+STOCK_PAGE_SIZE-1)//STOCK_PAGE_SIZE
    page=max(0,min(page,pages-1))
    start=page*STOCK_PAGE_SIZE; end=min(start+STOCK_PAGE_SIZE,total)
    buttons=[]
    for sku in SKU_LIST[start:end]:
        nm=SKU_NAME_CACHE.get(sku,f"SKU {sku}")
        buttons.append([InlineKeyboardButton(text=f"{nm[:48]} (SKU {sku})", callback_data=f"sku:{sku}")])
    nav=[]
    if page>0: nav.append(InlineKeyboardButton(text="¬´", callback_data=f"stockpage:{page-1}"))
    nav.append(InlineKeyboardButton(text=f"{page+1}/{pages}", callback_data="noop"))
    if page<pages-1: nav.append(InlineKeyboardButton(text="¬ª", callback_data=f"stockpage:{page+1}"))
    buttons.append(nav)
    buttons.append([InlineKeyboardButton(text="–ê–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ", callback_data="menu_autobook")])
    return InlineKeyboardMarkup(inline_keyboard=buttons)

# ===== Callbacks =====
@dp.callback_query(F.data.startswith("stockpage:"))
async def cb_stock_page(c:CallbackQuery):
    ensure_admin(c.from_user.id)
    try: page=int(c.data.split(":")[1])
    except Exception:
        await c.answer(); return
    to_fetch=skus_needing_names()
    if to_fetch:
        prev=len(SKU_NAME_CACHE); mp,_=await ozon_product_names_by_sku(to_fetch)
        SKU_NAME_CACHE.update(mp); save_cache_if_needed(prev)
    kb=build_stock_page(page)
    try: await c.message.edit_reply_markup(reply_markup=kb)
    except Exception: await c.message.answer("–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Ç–æ–≤–∞—Ä–æ–≤:", reply_markup=kb)
    await c.answer()

@dp.callback_query(F.data=="noop")
async def cb_noop(c:CallbackQuery):
    await c.answer()

@dp.callback_query(F.data=="action:reanalyze")
async def cb_reanalyze(c:CallbackQuery):
    ensure_admin(c.from_user.id)
    await handle_analyze(c.message.chat.id, verbose=False)
    await c.answer()

@dp.callback_query(F.data.startswith("filter:"))
async def cb_filter(c:CallbackQuery):
    ensure_admin(c.from_user.id)
    mode=c.data.split(":")[1]
    cache=LAST_DEFICIT_CACHE.get(c.message.chat.id)
    if not cache:
        await c.answer("–ù–µ—Ç –∫—ç—à–∞ ‚Äî –∞–Ω–∞–ª–∏–∑‚Ä¶")
        await handle_analyze(c.message.chat.id, verbose=False)
        return
    flat=cache["flat"]
    rep=rebuild_filtered_report(flat, mode, BOT_STATE.get("view_mode", DEFAULT_VIEW_MODE))
    kb=deficit_filters_kb()
    await send_long(c.message.chat.id, rep, kb=kb)
    await c.answer()

@dp.callback_query(F.data.startswith("sku:"))
async def cb_sku(c:CallbackQuery):
    ensure_admin(c.from_user.id)
    try: sku=int(c.data.split(":")[1])
    except Exception:
        await c.answer(); return
    rows, err=await ozon_stock_fbo(SKU_LIST)
    if err:
        await c.message.answer(f"–û—à–∏–±–∫–∞ Ozon API: {html.escape(err)}"); await c.answer(); return
    ccache=build_consumption_cache()
    agg=aggregate_rows(rows)
    if sku not in agg:
        await c.message.answer("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ SKU."); await c.answer(); return
    nm=SKU_NAME_CACHE.get(sku)
    if not nm or nm==f"SKU {sku}" or nm.lower().startswith("demo sku"):
        prev=len(SKU_NAME_CACHE); mp,_=await ozon_product_names_by_sku([sku])
        SKU_NAME_CACHE.update(mp); save_cache_if_needed(prev)
    name=SKU_NAME_CACHE.get(sku,f"SKU {sku}")
    lines=[f"{EMOJI_BOX} <b>{html.escape(name)} (SKU {sku})</b>", SEP_THIN]
    for wkey, info in sorted(agg[sku].items()):
        qty=info["qty"]; st=evaluate_position_cached(sku,wkey,qty,ccache)
        cov=qty/st["norm"] if st["norm"] else 0
        bar, sev=coverage_bar(cov)
        status=EMOJI_WARN if st["is_low"] else EMOJI_OK
        hist="hist" if st["history_used"] else "min"
        lines.append(f"‚Ä¢ {html.escape(info['warehouse_name'])}: {qty} | norm {st['norm']} | target {st['target']} {status}\n  {bar} {sev} [{hist}]")
    await send_long(c.message.chat.id, "\n".join(lines))
    await c.answer()

@dp.callback_query(F.data.startswith("wh:"))
async def cb_wh(c:CallbackQuery):
    ensure_admin(c.from_user.id)
    wkey=c.data.split(":",1)[1]
    rows, err=await ozon_stock_fbo(SKU_LIST)
    if err:
        await c.message.answer(f"–û—à–∏–±–∫–∞ Ozon API: {html.escape(err)}"); await c.answer(); return
    ccache=build_consumption_cache()
    agg=aggregate_rows(rows)
    lines=[f"{EMOJI_WH} <b>–°–∫–ª–∞–¥ {html.escape(wkey)}</b>", SEP_THIN]
    present=False
    for sku in sorted(agg.keys()):
        if wkey in agg[sku]:
            present=True
            info=agg[sku][wkey]
            nm=SKU_NAME_CACHE.get(sku)
            if not nm or nm==f"SKU {sku}" or nm.lower().startswith("demo sku"):
                prev=len(SKU_NAME_CACHE); mp,_=await ozon_product_names_by_sku([sku])
                SKU_NAME_CACHE.update(mp); save_cache_if_needed(prev)
            name=SKU_NAME_CACHE.get(sku,f"SKU {sku}")
            qty=info["qty"]; st=evaluate_position_cached(sku,wkey,qty,ccache)
            cov=qty/st["norm"] if st["norm"] else 0; bar, sev=coverage_bar(cov)
            status=EMOJI_WARN if st["is_low"] else EMOJI_OK
            lines.append(f"{html.escape(name)} (SKU {sku}): {qty} | norm {st['norm']} | target {st['target']} {status}\n  {bar} {sev}")
    if not present: lines.append("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–∫–ª–∞–¥—É.")
    await send_long(c.message.chat.id, "\n".join(lines))
    await c.answer()

@dp.callback_query(F.data.startswith("cluster:"))
async def cb_cluster(c:CallbackQuery):
    ensure_admin(c.from_user.id)
    cname=c.data.split(":",1)[1]
    cl_sec=FACT_INDEX.get("cluster",{}); sku_sec=FACT_INDEX.get("sku",{})
    if not cl_sec:
        await c.message.answer("–ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å –ø—É—Å—Ç. /analyze"); await c.answer(); return
    rep=build_cluster_detail(cname, cl_sec, sku_sec)
    await send_long(c.message.chat.id, rep)
    await c.answer()

@dp.callback_query(F.data=="chatmode:toggle")
async def cb_chatmode_toggle(c:CallbackQuery):
    ensure_admin(c.from_user.id)
    cur=BOT_STATE.get("chat_mode","fact"); new="general" if cur=="fact" else "fact"
    BOT_STATE["chat_mode"]=new; save_state()
    buttons=[[InlineKeyboardButton(text="üîÅ –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å —Ä–µ–∂–∏–º", callback_data="chatmode:toggle")],
             [InlineKeyboardButton(text="–ê–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ", callback_data="menu_autobook")]]
    await c.message.edit_text(build_html([
        "–†–µ–∂–∏–º —á–∞—Ç–∞ –ø–µ—Ä–µ–∫–ª—é—á—ë–Ω.",
        f"–¢–µ–ø–µ—Ä—å: <b>{new.upper()}</b>.",
        "–ù–∞–∂–º–∏ –µ—â—ë —Ä–∞–∑ –¥–ª—è —Å–º–µ–Ω—ã."
    ]), reply_markup=InlineKeyboardMarkup(inline_keyboard=buttons))
    await c.answer()

# ===== Overview & Menus =====
def main_menu_kb()->ReplyKeyboardMarkup:
    return ReplyKeyboardMarkup(keyboard=[
        [KeyboardButton(text="üîß –ê–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ"), KeyboardButton(text="üìÑ –ó–∞—è–≤–∫–∏")],
        [KeyboardButton(text="üîç –ê–Ω–∞–ª–∏–∑"), KeyboardButton(text="üì£ –û—Ç—á—ë—Ç —Å–µ–π—á–∞—Å")],
        [KeyboardButton(text="üì¶ –¢–æ–≤–∞—Ä—ã"), KeyboardButton(text="üè¨ –°–∫–ª–∞–¥—ã"), KeyboardButton(text="üó∫ –ö–ª–∞—Å—Ç–µ—Ä—ã")],
        [KeyboardButton(text="‚öô –†–µ–∂–∏–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"), KeyboardButton(text="üß™ –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞"), KeyboardButton(text="üîÑ –°–±—Ä–æ—Å –∫—ç—à–∞")],
        [KeyboardButton(text="ü§ñ AI —á–∞—Ç"), KeyboardButton(text="‚ùå –û—Ç–º–µ–Ω–∞")],
    ], resize_keyboard=True)

def start_overview()->str:
    rows=[
        ("üîç –ê–Ω–∞–ª–∏–∑","–ü–µ—Ä–µ—Å—á—ë—Ç –¥–µ—Ñ–∏—Ü–∏—Ç–æ–≤ / –∏–Ω–¥–µ–∫—Å–∞"),
        ("üì£ –û—Ç—á—ë—Ç —Å–µ–π—á–∞—Å","–ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑"),
        ("üì¶ –¢–æ–≤–∞—Ä—ã","–°–ø–∏—Å–æ–∫ SKU"),
        ("üè¨ –°–∫–ª–∞–¥—ã","–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–∫–ª–∞–¥–∞–º"),
        ("üó∫ –ö–ª–∞—Å—Ç–µ—Ä—ã","–ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –≥—Ä—É–ø–ø–∞–º"),
        ("üìÑ –ó–∞—è–≤–∫–∏","–ñ—É—Ä–Ω–∞–ª —Å—Ç–∞—Ç—É—Å–æ–≤ –∑–∞—è–≤–æ–∫"),
        ("‚öô –†–µ–∂–∏–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è","FULL / COMPACT"),
        ("üß™ –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞","–°—Ç–∞—Ç—É—Å—ã –∏ —Ç–æ–ø—ã"),
        ("üîÑ –°–±—Ä–æ—Å –∫—ç—à–∞","–û—á–∏—Å—Ç–∫–∞ –∏–º—ë–Ω SKU"),
        ("ü§ñ AI —á–∞—Ç","FACT/GENERAL"),
        ("üîß –ê–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ","–ü–æ—Å—Ç–∞–≤–∫–∞ (–º–∞—Å—Ç–µ—Ä)"),
        ("‚ùå –û—Ç–º–µ–Ω–∞","–í—ã—Ö–æ–¥ –∏–∑ AI —á–∞—Ç–∞"),
    ]
    lines=[]
    header=f"{'‚ïê'*25}  {EMOJI_INFO} –û–ë–ó–û–† –ë–û–¢–ê  {'‚ïê'*25}"
    lines.append(header)
    lines.append(f"–í–µ—Ä—Å–∏—è: {VERSION} | –†–µ–∂–∏–º: {BOT_STATE.get('chat_mode','?').upper()} | –°—Ç–∏–ª—å: {'ON' if BOT_STATE.get('style_enabled') else 'OFF'}")
    lines.append(SEP_THIN)
    cmds=["autobook","analyze","stock","warehouses","clusters","supplies","diag","facts_info","ai","ask","chat_mode",
          "style_toggle","cluster_map","autobook_diag"]
    lines.append("–ö–æ–º–∞–Ω–¥—ã: "+ " /".join(f"/{c}" for c in cmds))
    lines.append(SEP_THIN)
    lines.append("–ö–Ω–æ–ø–∫–∏:")
    ml=max(len(k) for k,_ in rows)
    for k,d in rows:
        lines.append(f" {k}{' '*(ml-len(k))} ‚îÇ {d}")
    lines.append(SEP_THIN)
    lines.append(f"–ê–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: {'–≤–Ω–µ—à–Ω–∏–π –º–æ–¥—É–ª—å –ø–æ–¥–∫–ª—é—á—ë–Ω' if AUTOBOOK_ENABLED else 'fallback –ø–æ–ª–Ω—ã–π –º–∞—Å—Ç–µ—Ä'}")
    lines.append("AI: FACT (–¥–∞–Ω–Ω—ã–µ) / GENERAL (—Å–≤–æ–±–æ–¥–Ω—ã–π).")
    lines.append("‚ïê"*len(header))
    return build_html(lines)

def version_info()->str:
    import sys
    return (f"–í–µ—Ä—Å–∏—è: {VERSION}\nPython: {sys.version.split()[0]}\nSnapshots: {len(HISTORY_CACHE)}\nIndex SKU: {len(FACT_INDEX.get('sku', {}))}\n"
            f"Clusters: {len(FACT_INDEX.get('cluster', {}))}\nChatMode: {BOT_STATE.get('chat_mode')} Style:{BOT_STATE.get('style_enabled')} "
            f"Autobook={'external' if AUTOBOOK_ENABLED else 'fallback'}")

# ===== Diagnostics & Supplies =====
def build_diag_report()->str:
    inv=FACT_INDEX.get("inventory_overview",{})
    sku_section=FACT_INDEX.get("sku",{})
    top_def=FACT_INDEX.get("top_deficits",[])
    top_wh=FACT_INDEX.get("top_warehouses",[])
    top_cl=FACT_INDEX.get("top_clusters",[])
    cov={"<25":0,"25-50":0,"50-80":0,"80-100":0,"100+":0}
    for _, info in sku_section.items():
        cv=info["worst_coverage"]
        if cv<0.25: cov["<25"]+=1
        elif cv<0.5: cov["25-50"]+=1
        elif cv<0.8: cov["50-80"]+=1
        elif cv<1: cov["80-100"]+=1
        else: cov["100+"]+=1
    lines=[]
    s=lambda t:f"¬ß¬ßB¬ß¬ß{t}¬ß¬ßEB¬ß¬ß"
    lines+=[f"{EMOJI_DIAG} {s('–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê')} ({time.strftime('%H:%M:%S')})",SEP_BOLD,
            f"{EMOJI_INFO} –í–µ—Ä—Å–∏—è {VERSION} | ChatMode={BOT_STATE.get('chat_mode')} | Style={'ON' if BOT_STATE.get('style_enabled') else 'OFF'} "
            f"| Autobook={'external' if AUTOBOOK_ENABLED else 'fallback'}",
            f"{EMOJI_CLOUD} Snapshot {FACT_INDEX.get('snapshot_ts','-')} | SKU_index={inv.get('total_sku','-')} | Clusters={len(FACT_INDEX.get('cluster',{}))}",
            ""]
    lines+=[f"{EMOJI_INV} {s('–ò–Ω–≤–µ–Ω—Ç–∞—Ä—å')}",SEP_BOLD,
            f"–ü–æ–∫—Ä—ã—Ç–∏–µ(worst): <25={cov['<25']} 25‚Äì50={cov['25-50']} 50‚Äì80={cov['50-80']} 80‚Äì100={cov['80-100']} ‚â•100={cov['100+']}",
            ""]
    lines+=[f"{EMOJI_TARGET} {s('Top –¥–µ—Ñ–∏—Ü–∏—Ç—ã')}",SEP_BOLD]
    if top_def:
        for td in top_def[:DIAG_TOP_DEFICITS]:
            lines.append(f"SKU {td['sku']} {td['name'][:30]} cov={td['coverage']:.2f} need={td['deficit_need']}")
    else:
        lines.append("–ù–µ—Ç –¥–µ—Ñ–∏—Ü–∏—Ç–æ–≤.")
    lines.append("")
    lines+=[f"{EMOJI_WH} {s('Top —Å–∫–ª–∞–¥—ã')}",SEP_BOLD]
    if top_wh:
        for w in top_wh[:DIAG_TOP_WAREHOUSES]:
            lines.append(f"{w['name'][:30]} need={w['total_need']} def_need={w['deficit_need']}")
    else:
        lines.append("–ù–µ—Ç —Å–∫–ª–∞–¥–æ–≤.")
    lines.append("")
    lines+=[f"{EMOJI_CLUSTER} {s('Top –∫–ª–∞—Å—Ç–µ—Ä—ã')}",SEP_BOLD]
    if top_cl:
        for c in top_cl[:DIAG_TOP_CLUSTERS]:
            lines.append(f"{c['name'][:30]} need={c['total_need']} def_need={c['deficit_need']}")
    else:
        lines.append("–ù–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.")
    lines.append("")
    lines+=[f"{EMOJI_PERF} {s('–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å')}",SEP_BOLD,
            f"API {LAST_API_LATENCY_MS:.0f}ms | –ê–Ω–∞–ª–∏–∑ {LAST_ANALYZE_MS:.0f}ms | –û—à–∏–±–∫–∞={LAST_ANALYZE_ERROR or '-'}",
            f"Snapshots={len(HISTORY_CACHE)} | –ö—ç—à –æ—Ç–≤–µ—Ç–æ–≤={len(ANSWER_CACHE)}"]
    return build_html(lines)

def build_supplies_list(chat_id:int, limit:int=40)->str:
    arr=SUPPLY_EVENTS.get(str(chat_id)) or []
    if not arr:
        return build_html([f"{EMOJI_LIST} –ù–µ—Ç —Å–æ–±—ã—Ç–∏–π –ø–æ –∑–∞—è–≤–∫–∞–º. –°–æ–∑–¥–∞–π—Ç–µ –∑–∞—è–≤–∫—É –≤ ¬´–ê–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ¬ª."])
    lines=[f"{EMOJI_LIST} ¬ß¬ßB¬ß¬ß–ñ—É—Ä–Ω–∞–ª –∑–∞—è–≤–æ–∫ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ {min(limit,len(arr))})¬ß¬ßEB¬ß¬ß",SEP_THIN]
    for e in arr[-limit:]:
        ts=time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(e.get("ts", int(time.time()))))
        if e.get("type")=="text":
            lines.append(f"[{ts}] {e.get('text','')}")
        elif e.get("type")=="file":
            lines.append(f"[{ts}] —Ñ–∞–π–ª: {e.get('file','?')} {('‚Äî '+e.get('caption')) if e.get('caption') else ''}")
        else:
            lines.append(f"[{ts}] {e}")
    return build_html(lines)

# ===== AI Dispatch =====
async def answer_and_send_fact(chat_id:int, question:str):
    raw, mode=await llm_fact_answer(question)
    styled=style_ai_answer(question, raw, mode, True)
    await send_long(chat_id, styled)

async def answer_and_send_general(chat_id:int, question:str):
    raw, mode=await llm_general_answer(chat_id, question)
    styled=style_ai_answer(question, raw, mode, False)
    await send_long(chat_id, styled)

# ===== Commands =====
@dp.message(Command("version"))
async def cmd_version(m:Message):
    ensure_admin(m.from_user.id); await m.answer(version_info())

@dp.message(Command("help"))
@dp.message(Command("start"))
async def cmd_start(m:Message):
    ensure_admin(m.from_user.id)
    await m.answer(f"{EMOJI_OK} –ë–æ—Ç –∞–∫—Ç–∏–≤–µ–Ω. –í–µ—Ä—Å–∏—è {VERSION}.", reply_markup=main_menu_kb())
    kb_rows=[[InlineKeyboardButton(text="–ê–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ", callback_data="menu_autobook")]]
    await send_long(m.chat.id, start_overview(), kb=InlineKeyboardMarkup(inline_keyboard=kb_rows))

@dp.message(Command("autobook"))
async def cmd_autobook(m:Message, state:FSMContext):
    ensure_admin(m.from_user.id)
    if AUTOBOOK_ENABLED:
        await m.answer("üß© –ó–∞–ø—É—Å–∫ –≤–Ω–µ—à–Ω–µ–≥–æ –º–∞—Å—Ç–µ—Ä–∞‚Ä¶", reply_markup=InlineKeyboardMarkup(inline_keyboard=[
            [InlineKeyboardButton(text="–ù–∞—á–∞—Ç—å", callback_data="menu_autobook")]
        ]))
    else:
        if not SKU_LIST:
            await m.answer("SKU_LIST –ø—É—Å—Ç. –ó–∞–¥–∞–π—Ç–µ –≤ .env."); return
        to_fetch=skus_needing_names()
        if to_fetch:
            prev=len(SKU_NAME_CACHE); mp,_=await ozon_product_names_by_sku(to_fetch)
            SKU_NAME_CACHE.update(mp); save_cache_if_needed(prev)
        await state.set_state(AutobookStates.picking_sku)
        kb=_ab_sku_page(0)
        await m.answer("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–æ–≤–∞—Ä –¥–ª—è –ø–æ—Å—Ç–∞–≤–∫–∏:", reply_markup=kb)

@dp.message(Command("supplies"))
async def cmd_supplies(m:Message):
    ensure_admin(m.from_user.id)
    rep=build_supplies_list(m.chat.id, limit=40)
    await send_long(m.chat.id, rep)

@dp.message(Command("cluster_map"))
async def cmd_cluster_map(m:Message):
    ensure_admin(m.from_user.id)
    if CLUSTER_MAP:
        lines=["¬ß¬ßB¬ß¬ß–û–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ (ENV) –∫–ª–∞—Å—Ç–µ—Ä—ã¬ß¬ßEB¬ß¬ß", SEP_THIN]
        for k,v in CLUSTER_MAP.items():
            lines.append(f"{k} => {v}")
        lines.append(SEP_THIN)
    else:
        lines=["¬ß¬ßB¬ß¬ßENV –Ω–µ –∑–∞–¥–∞–Ω ‚Äî —ç–≤—Ä–∏—Å—Ç–∏–∫–∏¬ß¬ßEB¬ß¬ß","–ù–µ–æ–ø–æ–∑–Ω–∞–Ω–Ω—ã–µ -> '–ü—Ä–æ—á–∏–µ'."]
    await send_long(m.chat.id, build_html(lines))

@dp.message(Command("health"))
async def cmd_health(m:Message):
    ensure_admin(m.from_user.id)
    flags=[]
    if LAST_API_LATENCY_MS>HEALTH_WARN_LATENCY_MS: flags.append("API –º–µ–¥–ª–µ–Ω–Ω–æ")
    if LAST_ANALYZE_MS>HEALTH_WARN_LATENCY_MS: flags.append("–ê–Ω–∞–ª–∏–∑ –º–µ–¥–ª–µ–Ω–Ω–æ")
    status="OK" if not flags else " | ".join(flags)
    lines=["¬ß¬ßB¬ß¬ßHEALTH¬ß¬ßEB¬ß¬ß",
           f"API {LAST_API_LATENCY_MS:.0f}ms  –ê–Ω–∞–ª–∏–∑ {LAST_ANALYZE_MS:.0f}ms",
           f"Snapshots={len(HISTORY_CACHE)} SKU_index={len(FACT_INDEX.get('sku', {}))} Clusters={len(FACT_INDEX.get('cluster', {}))}",
           f"ChatMode={BOT_STATE.get('chat_mode')} Style={'ON' if BOT_STATE.get('style_enabled') else 'OFF'} Autobook={'external' if AUTOBOOK_ENABLED else 'fallback'}",
           f"Status={status}"]
    await send_long(m.chat.id, build_html(lines))

@dp.message(Command("view_mode"))
async def cmd_view_mode(m:Message):
    ensure_admin(m.from_user.id)
    BOT_STATE["view_mode"]="COMPACT" if BOT_STATE.get("view_mode")=="FULL" else "FULL"; save_state()
    await m.answer(f"–†–µ–∂–∏–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è: {BOT_STATE['view_mode']}")

@dp.message(Command("style_toggle"))
async def cmd_style_toggle(m:Message):
    ensure_admin(m.from_user.id)
    BOT_STATE["style_enabled"]=not BOT_STATE.get("style_enabled", True); save_state()
    await m.answer(f"–°—Ç–∏–ª–∏–∑–∞—Ü–∏—è AI: {'ON' if BOT_STATE['style_enabled'] else 'OFF'}")

@dp.message(Command("chat_mode"))
async def cmd_chat_mode(m:Message):
    ensure_admin(m.from_user.id)
    mode=BOT_STATE.get("chat_mode","fact").upper()
    buttons=[[InlineKeyboardButton(text="üîÅ –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å —Ä–µ–∂–∏–º", callback_data="chatmode:toggle")],
             [InlineKeyboardButton(text="–ê–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ", callback_data="menu_autobook")]]
    await m.answer(build_html([
        "¬ß¬ßB¬ß¬ß–†–µ–∂–∏–º —á–∞—Ç–∞¬ß¬ßEB¬ß¬ß",
        f"–¢–µ–∫—É—â–∏–π: <b>{mode}</b>",
        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /fact /general –∏–ª–∏ –∫–Ω–æ–ø–∫—É."
    ]), reply_markup=InlineKeyboardMarkup(inline_keyboard=buttons))

@dp.message(Command("fact"))
async def cmd_fact(m:Message):
    ensure_admin(m.from_user.id)
    BOT_STATE["chat_mode"]="fact"; save_state()
    await m.answer("–†–µ–∂–∏–º —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: FACT")

@dp.message(Command("general"))
async def cmd_general(m:Message):
    ensure_admin(m.from_user.id)
    BOT_STATE["chat_mode"]="general"; save_state()
    await m.answer("–†–µ–∂–∏–º —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: GENERAL")

@dp.message(Command("chat"))
async def cmd_chat(m:Message):
    ensure_admin(m.from_user.id)
    q=m.text.partition(" ")[2].strip()
    if not q: return await m.answer("–§–æ—Ä–º–∞—Ç: /chat <—Å–æ–æ–±—â–µ–Ω–∏–µ>")
    await m.answer(f"{EMOJI_CHAT} –û–±—â–∞—é—Å—å‚Ä¶")
    raw,mode=await llm_general_answer(m.chat.id,q)
    styled=style_ai_answer(q,raw,mode,False)
    await send_long(m.chat.id, styled)

@dp.message(Command("refresh"))
async def cmd_refresh(m:Message):
    ensure_admin(m.from_user.id)
    SKU_NAME_CACHE.clear(); save_cache_if_needed(0)
    await m.answer("–ö—ç—à –∏–º—ë–Ω SKU –æ—á–∏—â—ë–Ω.")

@dp.message(Command("analyze"))
async def cmd_analyze(m:Message):
    ensure_admin(m.from_user.id)
    await handle_analyze(m.chat.id)

@dp.message(Command("force_notify"))
async def cmd_force_notify(m:Message):
    ensure_admin(m.from_user.id)
    await m.answer("–§–æ—Ä–º–∏—Ä—É—é –æ—Ç—á—ë—Ç‚Ä¶")
    await daily_notify_job()
    await m.answer("–ì–æ—Ç–æ–≤–æ.")

@dp.message(Command("diag"))
async def cmd_diag(m:Message):
    ensure_admin(m.from_user.id)
    rep=build_diag_report()
    await send_long(m.chat.id, rep)

@dp.message(Command("diag_env"))
async def cmd_diag_env(m:Message):
    ensure_admin(m.from_user.id)
    def mask(v:str)->str:
        if not v: return "(empty)"
        if len(v)<8: return v[0]+"***"
        return v[:4]+"****"+v[-4:]
    lines=["<b>ENV</b>",
           f"VERSION={VERSION}",
           f"OZON_CLIENT_ID={'yes' if OZON_CLIENT_ID else 'no'}",
           f"OZON_API_KEY={mask(OZON_API_KEY)}",
           f"LLM_PROVIDER={LLM_PROVIDER}",
           f"GIGACHAT_SCOPE={GIGACHAT_SCOPE}",
           f"GENERAL_TEMP={LLM_GENERAL_TEMPERATURE}",
           f"CHAT_MODE={BOT_STATE.get('chat_mode')}",
           f"STYLE={BOT_STATE.get('style_enabled')}",
           f"INVENTORY_SAMPLE={LLM_INVENTORY_SAMPLE_SKU}",
           f"FULL_DETAIL_SKU={LLM_FULL_DETAIL_SKU}",
           f"FULL_DETAIL_WH={LLM_FULL_DETAIL_WAREHOUSES}",
           f"FACT_SOFT_LIMIT={LLM_FACT_SOFT_LIMIT_CHARS}",
           f"STOCK_PAGE_SIZE={STOCK_PAGE_SIZE}",
           f"CLUSTER_MAP={'yes' if CLUSTER_MAP else 'heuristic'}",
           f"CLUSTER_COUNT={len(FACT_INDEX.get('cluster', {}))}",
           f"AUTOBOOK={'external' if AUTOBOOK_ENABLED else 'fallback'}",
           f"DEFAULT_DROPOFF_ID={DEFAULT_DROPOFF_ID or '(none)'}",
           f"DAYS={DAYS_ENV} DISABLE_TS_FALLBACK={DISABLE_TS_FALLBACK}"]
    await send_long(m.chat.id, build_html(lines))

@dp.message(Command("stock"))
async def cmd_stock(m:Message):
    ensure_admin(m.from_user.id)
    to_fetch=skus_needing_names()
    if to_fetch:
        prev=len(SKU_NAME_CACHE); mp,_=await ozon_product_names_by_sku(to_fetch)
        SKU_NAME_CACHE.update(mp); save_cache_if_needed(prev)
    kb=build_stock_page(0)
    await m.answer(f"{EMOJI_BOX} –¢–æ–≤–∞—Ä—ã:", reply_markup=kb)

@dp.message(Command("warehouses"))
async def cmd_warehouses(m:Message):
    ensure_admin(m.from_user.id)
    rows, err=await ozon_stock_fbo(SKU_LIST)
    if err: return await m.answer(f"–û—à–∏–±–∫–∞ Ozon API: {html.escape(err)}")
    agg=aggregate_rows(rows)
    wh_map={}
    for wmap in agg.values():
        for wk,info in wmap.items():
            wh_map.setdefault(wk, info["warehouse_name"])
    if not wh_map: return await m.answer("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö.")
    kb_rows=[[InlineKeyboardButton(text=(nm or wk)[:60], callback_data=f"wh:{wk}")]
             for wk,nm in sorted(wh_map.items(), key=lambda x:x[1].lower())]
    kb_rows.append([InlineKeyboardButton(text="–ê–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ", callback_data="menu_autobook")])
    kb=InlineKeyboardMarkup(inline_keyboard=kb_rows)
    await m.answer(f"{EMOJI_WH} –°–∫–ª–∞–¥—ã:", reply_markup=kb)

@dp.message(Command("clusters"))
async def cmd_clusters(m:Message):
    ensure_admin(m.from_user.id)
    if not FACT_INDEX.get("cluster"):
        return await m.answer("–ö–ª–∞—Å—Ç–µ—Ä—ã –µ—â—ë –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã. /analyze")
    kb_list=[]
    for cname in sorted(FACT_INDEX["cluster"].keys(), key=lambda c: FACT_INDEX["cluster"][c]["total_need"], reverse=True):
        kb_list.append([InlineKeyboardButton(text=cname[:40], callback_data=f"cluster:{cname}")])
    kb_list.append([InlineKeyboardButton(text="–ê–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ", callback_data="menu_autobook")])
    kb=InlineKeyboardMarkup(inline_keyboard=kb_list)
    await m.answer(f"{EMOJI_CLUSTER} –ö–ª–∞—Å—Ç–µ—Ä—ã:", reply_markup=kb)

@dp.message(Command("ai"))
async def cmd_ai(m:Message):
    ensure_admin(m.from_user.id)
    q=m.text.partition(" ")[2].strip() or "–í–µ—Å—å –æ–±—ä—ë–º –¥–∞–Ω–Ω—ã—Ö"
    await m.answer(f"{EMOJI_AI} –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é‚Ä¶")
    await answer_and_send_fact(m.chat.id, q)

@dp.message(Command("ask"))
async def cmd_ask(m:Message):
    ensure_admin(m.from_user.id)
    q=m.text.partition(" ")[2].strip()
    if not q: return await m.answer("–§–æ—Ä–º–∞—Ç: /ask <–≤–æ–ø—Ä–æ—Å>")
    await m.answer(f"{EMOJI_AI} –ê–Ω–∞–ª–∏–∑ —Ñ–∞–∫—Ç–æ–≤‚Ä¶")
    await answer_and_send_fact(m.chat.id, q)

@dp.message(Command("ask_raw"))
async def cmd_ask_raw(m:Message):
    ensure_admin(m.from_user.id)
    q=m.text.partition(" ")[2].strip() or "–í–µ—Å—å –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö"
    facts, mode=build_facts_block(q)
    facts_colored=[]
    for ln in facts.splitlines():
        if "cov=" in ln:
            mc=re.search(r"cov=([0-9.]+)", ln)
            if mc:
                cv=float(mc.group(1)); color="üü•" if cv<0.25 else "üüß" if cv<0.5 else "üü®" if cv<0.8 else "üü©"
                ln=f"{color} {ln}"
        facts_colored.append(ln)
    facts="\n".join(facts_colored)
    out=[f"MODE={mode}","----- FACTS -----", facts[:3800]]
    if len(facts)>3800: out.append("...(—É—Å–µ—á–µ–Ω–æ)")
    await send_long(m.chat.id, build_html(out))

@dp.message(Command("facts_info"))
async def cmd_facts_info(m:Message):
    ensure_admin(m.from_user.id)
    if not FACT_INDEX: return await m.answer("–ò–Ω–¥–µ–∫—Å –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω. /analyze")
    inv=FACT_INDEX.get("inventory_overview", {})
    lines=["¬ß¬ßB¬ß¬ßFACTS INFO¬ß¬ßEB¬ß¬ß",
           f"updated_ts={FACT_INDEX.get('updated_ts')}",
           f"snapshot_ts={FACT_INDEX.get('snapshot_ts')}",
           f"SKU indexed={inv.get('total_sku','?')}",
           f"Warehouses={len(FACT_INDEX.get('warehouse', {}))}",
           f"Clusters={len(FACT_INDEX.get('cluster', {}))}",
           f"Top deficits={len(FACT_INDEX.get('top_deficits', []))}",
           f"Top warehouses={len(FACT_INDEX.get('top_warehouses', []))}",
           f"Top clusters={len(FACT_INDEX.get('top_clusters', []))}",
           f"Answer cache size={len(ANSWER_CACHE)}"]
    await send_long(m.chat.id, build_html(lines))

@dp.message(Command("facts_dump"))
async def cmd_facts_dump(m:Message):
    ensure_admin(m.from_user.id)
    if not FACT_INDEX: return await m.answer("–ü—É—Å—Ç–æ.")
    clone=dict(FACT_INDEX)
    for sku, entry in clone.get("sku", {}).items():
        entry["warehouses"]=entry.get("warehouses", [])[:LLM_FULL_DETAIL_WAREHOUSES]
    dump=json.dumps(clone, ensure_ascii=False)
    if len(dump)>3900: dump=dump[:3900]+"...(—É—Å–µ—á–µ–Ω–æ)"
    await send_long(m.chat.id, build_html(["JSON:", dump]))

@dp.message(Command("ai_scope"))
async def cmd_ai_scope(m:Message):
    ensure_admin(m.from_user.id)
    tok=_GIGACHAT_TOKEN_MEM
    ttl=int(tok["expires_epoch"]-time.time()) if tok and tok.get("expires_epoch") else -1
    insecure=(_gigachat_verify_param() is False)
    lines=["¬ß¬ßB¬ß¬ßGigaChat —Å—Ç–∞—Ç—É—Å¬ß¬ßEB¬ß¬ß",
           f"Enabled={GIGACHAT_ENABLED} ChatMode={BOT_STATE.get('chat_mode')}",
           f"Token={'yes' if tok else 'no'} TTL={ttl if ttl>=0 else '-'}",
           f"SSL mode={GIGACHAT_SSL_MODE}{' (INSECURE!)' if insecure else ''}",
           f"Index SKU={len(FACT_INDEX.get('sku', {}))} Clusters={len(FACT_INDEX.get('cluster', {}))}",
           f"Answer cache={len(ANSWER_CACHE)}"]
    await send_long(m.chat.id, build_html(lines))

@dp.message(Command("ai_reset_token"))
async def cmd_ai_reset_token(m:Message):
    ensure_admin(m.from_user.id)
    global _GIGACHAT_TOKEN_MEM
    _GIGACHAT_TOKEN_MEM={}
    if GIGACHAT_TOKEN_CACHE_FILE.exists():
        try: GIGACHAT_TOKEN_CACHE_FILE.unlink()
        except Exception: pass
    await m.answer("–¢–æ–∫–µ–Ω —Å–±—Ä–æ—à–µ–Ω.")

@dp.message(Command("ai_diag_auth"))
async def cmd_ai_diag_auth(m:Message):
    ensure_admin(m.from_user.id)
    cid=_sanitize_cred(GIGACHAT_CLIENT_ID); sec=_sanitize_cred(GIGACHAT_CLIENT_SECRET)
    def mask(v:str):
        if not v: return "(empty)"
        if len(v)<=4: return v[0]+"***"
        return v[:2]+"***"+v[-2:]
    lines=["¬ß¬ßB¬ß¬ßGigaChat creds diag¬ß¬ßEB¬ß¬ß",
           f"CLIENT_ID len={len(cid)} masked={mask(cid)}",
           f"CLIENT_SECRET len={len(sec)} masked={mask(sec)}",
           f"Scope={GIGACHAT_SCOPE}",
           f"SSL_MODE={GIGACHAT_SSL_MODE}",
           f"Token_cached={'yes' if _token_valid(_GIGACHAT_TOKEN_MEM) else 'no'}"]
    await send_long(m.chat.id, build_html(lines))

def _run_shell(cmd:str, timeout=20)->Tuple[int,str,str]:
    try:
        p=subprocess.run(shlex.split(cmd),capture_output=True,text=True,timeout=timeout)
        return p.returncode,p.stdout,p.stderr
    except Exception as e:
        return 1,"",str(e)

@dp.message(Command("ai_tls_diag"))
async def cmd_ai_tls_diag(m:Message):
    ensure_admin(m.from_user.id)
    cmd="openssl s_client -showcerts -servername ngw.devices.sberbank.ru -connect ngw.devices.sberbank.ru:9443 < /dev/null"
    rc,out,err=_run_shell(cmd,timeout=25)
    snippet="\n".join(out.splitlines()[:30])
    lines=["¬ß¬ßB¬ß¬ßTLS –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞¬ß¬ßEB¬ß¬ß", f"rc={rc}",
           f"BEGIN_CERT count={out.count('BEGIN CERTIFICATE')}",
           f"SSL_MODE={GIGACHAT_SSL_MODE}",
           "----- SNIPPET -----", snippet or "(empty)"]
    await send_long(m.chat.id, build_html(lines))

@dp.message(Command("ai_auto_ca"))
async def cmd_ai_auto_ca(m:Message):
    ensure_admin(m.from_user.id)
    if not GIGACHAT_CA_CERT:
        return await m.answer("GIGACHAT_CA_CERT –Ω–µ –∑–∞–¥–∞–Ω.")
    cmd="openssl s_client -showcerts -servername ngw.devices.sberbank.ru -connect ngw.devices.sberbank.ru:9443 < /dev/null"
    rc,out,err=_run_shell(cmd,timeout=25)
    if rc!=0 or "BEGIN CERTIFICATE" not in out:
        return await m.answer(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ü–µ–ø–æ—á–∫—É rc={rc} err={err[:120]}")
    blocks=out.split("-----BEGIN CERTIFICATE-----"); certs=[]
    for blk in blocks:
        if "-----END CERTIFICATE-----" in blk:
            cert="-----BEGIN CERTIFICATE-----"+blk.split("-----END CERTIFICATE-----")[0]+"-----END CERTIFICATE-----\n"
            certs.append(cert)
    if not certs: return await m.answer("–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã.")
    bundle="".join(certs)
    try:
        Path(GIGACHAT_CA_CERT).parent.mkdir(parents=True, exist_ok=True)
        Path(GIGACHAT_CA_CERT).write_text(bundle, encoding="utf-8")
    except Exception as e:
        return await m.answer(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ CA: {e}")
    await m.answer(f"CA bundle –æ–±–Ω–æ–≤–ª—ë–Ω. –°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤: {bundle.count('BEGIN CERTIFICATE')}.")

@dp.message(Command("supply_purge_all"))
async def cmd_supply_purge_all(m:Message):
    ensure_admin(m.from_user.id)
    if not purge_all_tasks:
        return await m.answer("–§—É–Ω–∫—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")
    cnt=purge_all_tasks()
    await m.answer(f"–£–¥–∞–ª–µ–Ω–æ –∑–∞–¥–∞—á: {cnt}")

@dp.message(Command("supply_purge_stale"))
async def cmd_supply_purge_stale(m:Message):
    ensure_admin(m.from_user.id)
    if not purge_stale_nonfinal:
        return await m.answer("–§—É–Ω–∫—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")
    try: hours=int(m.text.strip().split(maxsplit=1)[1])
    except Exception: hours=48
    cnt=purge_stale_nonfinal(hours=hours)
    await m.answer(f"–£–¥–∞–ª–µ–Ω–æ –∑–∞–≤–∏—Å—à–∏—Ö –∑–∞–¥–∞—á –∑–∞ >{hours}—á: {cnt}")

# ===== FSM Chat =====
@dp.message(F.text=="ü§ñ AI —á–∞—Ç")
async def btn_ai_chat(m:Message, state:FSMContext):
    ensure_admin(m.from_user.id)
    if not GIGACHAT_ENABLED:
        return await m.answer("LLM –æ—Ç–∫–ª—é—á—ë–Ω.", reply_markup=main_menu_kb())
    await state.set_state(AIChatState.waiting)
    mode=BOT_STATE.get("chat_mode","fact")
    await m.answer(f"AI —á–∞—Ç –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω. –†–µ–∂–∏–º: {mode.upper()}.\n–ö–æ–º–∞–Ω–¥—ã: /fact /general /chat_mode /cancel\n–ù–∞–ø–∏—à–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ:",
                   reply_markup=main_menu_kb())

@dp.message(Command("cancel"))
@dp.message(F.text=="‚ùå –û—Ç–º–µ–Ω–∞")
async def cmd_cancel(m:Message, state:FSMContext):
    ensure_admin(m.from_user.id)
    await state.clear()
    await m.answer("AI —á–∞—Ç/–º–∞—Å—Ç–µ—Ä –∑–∞–≤–µ—Ä—à—ë–Ω.", reply_markup=main_menu_kb())

@dp.message(AIChatState.waiting)
async def ai_chat_waiting(m:Message, state:FSMContext):
    ensure_admin(m.from_user.id)
    q=(m.text or "").strip()
    if not q: return await m.answer("–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å. /cancel –¥–ª—è –≤—ã—Ö–æ–¥–∞.")
    mode=BOT_STATE.get("chat_mode","fact")
    await m.answer(f"{EMOJI_AI} –î—É–º–∞—é‚Ä¶ ({mode})")
    if mode=="general":
        raw,rmode=await llm_general_answer(m.chat.id,q)
        styled=style_ai_answer(q,raw,rmode,False)
    else:
        raw,rmode=await llm_fact_answer(q)
        styled=style_ai_answer(q,raw,rmode,True)
    if styled.count("\n")>AI_MAX_RENDER_LINES:
        parts=styled.splitlines()
        styled="\n".join(parts[:AI_MAX_RENDER_LINES])+"\n...(—É—Å–µ—á–µ–Ω–æ)"
    await send_long(m.chat.id, styled)
    await m.answer("–°–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –∏–ª–∏ /cancel.", reply_markup=main_menu_kb())

# ===== Button Aliases =====
BUTTON_ALIASES={
    "–∞–Ω–∞–ª–∏–∑": cmd_analyze,
    "–æ—Ç—á—ë—Ç —Å–µ–π—á–∞—Å": cmd_analyze,
    "—Ç–æ–≤–∞—Ä—ã": cmd_stock,
    "—Å–∫–ª–∞–¥—ã": cmd_warehouses,
    "–∫–ª–∞—Å—Ç–µ—Ä—ã": cmd_clusters,
    "–∑–∞—è–≤–∫–∏": cmd_supplies,
    "—Ä–µ–∂–∏–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è": cmd_view_mode,
    "–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞": cmd_diag,
    "—Å–±—Ä–æ—Å –∫—ç—à–∞": cmd_refresh,
}

@dp.message(StateFilter(None), F.text.regexp(r'^(?!\/)'))
async def text_buttons(m:Message, state:FSMContext):
    raw=(m.text or "").lower()
    for em in ["üîß","üîç","üì£","üì¶","üè¨","üó∫","‚öô","üß™","üîÑ","ü§ñ","‚ùå","üìÑ"]:
        raw=raw.replace(em.lower(),"")
    raw=raw.strip()
    if raw=="–∞–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ" or "–∞–≤—Ç–æ–±—Ä–æ–Ω–∏—Ä" in raw:
        ensure_admin(m.from_user.id); await cmd_autobook(m, state)
    elif raw in BUTTON_ALIASES:
        ensure_admin(m.from_user.id); await BUTTON_ALIASES[raw](m)

# ===== Jobs & Scheduler =====
async def snapshot_job():
    if time.time()-LAST_SNAPSHOT_TS<SNAPSHOT_MIN_REUSE_SECONDS: return
    rows, err=await ozon_stock_fbo(SKU_LIST)
    if err:
        log.warning("snapshot_job error: %s", err); return
    if not rows: log.warning("snapshot_job empty rows"); return
    append_snapshot(rows)
    to_fetch=skus_needing_names()
    if to_fetch:
        prev=len(SKU_NAME_CACHE); mp,_=await ozon_product_names_by_sku(to_fetch)
        SKU_NAME_CACHE.update(mp); save_cache_if_needed(prev)
    try:
        ccache=build_consumption_cache(); build_fact_index(rows, [], ccache)
    except Exception as e:
        log.warning("snapshot index build fail: %s", e)
    await flush_history_if_needed(force=True)

async def daily_notify_job():
    if ADMIN_ID is None: return
    async with ANALYZE_LOCK:
        rows, err=await ozon_stock_fbo(SKU_LIST)
        if err:
            await send_safe_message(ADMIN_ID,f"–û—à–∏–±–∫–∞ Ozon API: {html.escape(err)}"); return
        if time.time()-LAST_SNAPSHOT_TS>SNAPSHOT_MIN_REUSE_SECONDS:
            append_snapshot(rows); await flush_history_if_needed(force=True)
        ccache=build_consumption_cache()
        to_fetch=skus_needing_names()
        if to_fetch:
            prev=len(SKU_NAME_CACHE); mp,_=await ozon_product_names_by_sku(to_fetch)
            SKU_NAME_CACHE.update(mp); save_cache_if_needed(prev)
        report, flat=generate_deficit_report(rows, SKU_NAME_CACHE, ccache)
        LAST_DEFICIT_CACHE[ADMIN_ID]={"flat":flat,"timestamp":int(time.time()),"report":report,"raw_rows":rows,"consumption_cache":ccache}
        try: build_fact_index(rows, flat, ccache)
        except Exception as e: log.warning("FACT_INDEX daily build fail: %s", e)
        header=f"{EMOJI_NOTIFY} <b>–ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –æ—Ç—á—ë—Ç {DAILY_NOTIFY_HOUR:02d}:{DAILY_NOTIFY_MINUTE:02d}</b>\n"
        kb=deficit_filters_kb()
        await send_long(ADMIN_ID, header+report, kb=kb)
        await flush_history_if_needed()

async def maintenance_job():
    prune_history()
    await flush_history_if_needed()

async def init_snapshot():
    rows, err=await ozon_stock_fbo(SKU_LIST)
    if err:
        log.warning("init snapshot error: %s", err); return
    append_snapshot(rows)
    try:
        ccache=build_consumption_cache(); build_fact_index(rows, [], ccache)
    except Exception as e:
        log.warning("init index build fail: %s", e)
    await flush_history_if_needed(force=True)
    log.info("Initial snapshot rows=%d", len(rows))

def _try_register_supply_scheduler(scheduler: AsyncIOScheduler):
    try:
        register_supply_scheduler(scheduler, notify_text=supply_notify_text, notify_file=supply_notify_file, interval=SUPPLY_JOB_INTERVAL)
        log.info("Supply scheduler registered (notify_text/notify_file/interval).")
    except TypeError:
        try:
            register_supply_scheduler(scheduler, notify_text=supply_notify_text, notify_file=supply_notify_file)
            log.info("Supply scheduler registered (notify_text/notify_file).")
        except TypeError:
            try:
                register_supply_scheduler(scheduler)
                log.info("Supply scheduler registered (scheduler only).")
            except Exception as e:
                log.warning("register_supply_scheduler failed: %s", e)

def setup_scheduler()->AsyncIOScheduler:
    tz=ZoneInfo(TZ_NAME)
    scheduler=AsyncIOScheduler(timezone=tz)
    scheduler.add_job(snapshot_job, "interval",
                      minutes=max(1, SNAPSHOT_INTERVAL_MINUTES),
                      id="snapshot_job", max_instances=1, coalesce=True, misfire_grace_time=60)
    scheduler.add_job(maintenance_job, "interval",
                      minutes=max(5, HISTORY_PRUNE_EVERY_MINUTES),
                      id="maintenance_job", max_instances=1, coalesce=True, misfire_grace_time=60)
    scheduler.add_job(daily_notify_job, "cron",
                      hour=DAILY_NOTIFY_HOUR, minute=DAILY_NOTIFY_MINUTE,
                      id="daily_notify_job", max_instances=1, coalesce=True, misfire_grace_time=600)
    return scheduler

def _register_signal_handlers(loop: asyncio.AbstractEventLoop, scheduler: AsyncIOScheduler):
    def _graceful_stop():
        try:
            scheduler.shutdown(wait=False)
        except Exception:
            pass
        for task in asyncio.all_tasks(loop):
            if task is not asyncio.current_task(loop):
                task.cancel()
        try:
            loop.stop()
        except Exception:
            pass
    for sig in (signal.SIGINT, signal.SIGTERM):
        try:
            loop.add_signal_handler(sig, _graceful_stop)
        except NotImplementedError:
            pass

async def main():
    load_state()
    load_cache()
    load_history()
    if not HISTORY_CACHE:
        await init_snapshot()

    if AUTOBOOK_ENABLED and autobook_router is not None:
        try:
            dp.include_router(autobook_router)
            log.info("External autobook router included.")
        except Exception as e:
            log.warning("include_router failed: %s", e)

    scheduler=setup_scheduler()
    _try_register_supply_scheduler(scheduler)
    scheduler.start()
    log.info("Scheduler started. TZ=%s", TZ_NAME)

    _register_signal_handlers(asyncio.get_running_loop(), scheduler)

    log.info("Starting polling... Version=%s MOCK_MODE=%s DAYS_ENV=%s", VERSION, MOCK_MODE, DAYS_ENV)
    try:
        await dp.start_polling(bot)
    finally:
        try:
            scheduler.shutdown(wait=False)
        except Exception:
            pass

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass